% !TeX root = summary.tex

\chapter{Klassifikation}

\begin{itemize}
	\item Einordnen in die Welt, gesellschaftliche Konventionen, biologische Kategorien
	\item Komplex, was definiert ein Objekt und Beziehungen ? $\to$ nie 100\% $\to$ Wahrscheinlichkeit
\end{itemize}


\textbf{Pattern Recognition Overview}
\begin{itemize}
	\item Static Patterns (no time / order dependence)
	\begin{itemize}
		\item Supervised - Unsupervised
		\item Parametric - Non-Parametric
		\item Linear - Non-linear
	\end{itemize}
	\item \textbf{Classical Methods:} Bayes Classifier, k-nearest neigbhor
	\item \textbf{Connectionist Methods:} (Multilayer) Perceptron
\end{itemize}

\begin{description}
\item[Supervised Training] Klassen in Trainingsdaten (a priori) bekannt
\item[Unsupervised Training] Klassen nicht bekannt, Struktur  muss automatisch herausgefunden werden ($\to$ Clustering, Deep Learning)
\item[Parametric] Existenz einer Verteilungsfkt annehmen, Parameter abschätzen ($\to$ Gauss Klassifikation)
\item[Non-Parametric] Keine Verteilungsfkt, Fehler direkt aus Trainingsdaten ($\to$ $k$-nearest neighbor, Perzeptron)
\end{description}

\section{Schablonenanpassung\index{Schablonenanpassung} (\textsl{Template Matching})}
\begin{itemize}
\item Ähnlichkeit zum abgespeicherten Beispiel erkennen
\item Maß der Übereinstimmung ist der Absolutbetrag zwischen Muster und Schablone (zentriert an $(m,n)$): $$M_{f,g}(m,n) = \sum\limits_i \sum\limits_j f(i,j) g(i-m,j-n)$$
\item Abstand $E(m,n)$ ist definiert als Quadrat der Kreuzkorrelation: $$E_{f,g} (m,n) = M_{f,g}^2(m,n)$$
\item Normalisieren der Helligkeit $$M_{f,g}(m,n) = \frac{1}{(N - 1) \sigma_f \sigma_g} \sum_i \sum_j (f(i,j) - \textit{\=f})(g(i-m, j-n) - \textit{\=g})$$
\end{itemize}


\section{Bayes Entscheidungstheorie}

Bayes Regel\index{Bayes Regel}: $$P(\omega_j | x) = \frac{p(x | \omega_j) P(\omega_j)}{p(x)} \quad \textrm{wobei} \quad p(x) = \sum\limits_{j} p(x | \omega_j) P(\omega_j)$$
A priori / posteriori Wahrscheinlichkeit: $$P(\omega_j) / P(\omega_j | x)$$
Klassenbedingte Wahrscheinlichkeitsdichte: $$p(x | \omega_j)$$

\subsection{Zwei Klassen Fall}

$$P(error | x) = \left\{ \begin{array}{cl} P(\omega_1 | x) & \textrm{wenn wir uns für } \omega_2 \textrm{ entscheiden} \\ P(\omega_2 | x) & \textrm{sonst} \end{array} \right.$$
Der Fehler ist minimiert, wenn wir uns entscheiden für:
\begin{itemize}
\item $\omega_1$ wenn $P(\omega_1 | x) > P(\omega_2 | x)$ \\ $\omega_2$ sonst
\item $\omega_1$ wenn $p(x | \omega_1)P(\omega_1) > p(x | \omega_2)P(\omega_2)$ \\ $\omega_2$ sonst
\end{itemize}
Mehr Klassen:
\begin{itemize}
\item $\omega_i$ wenn $P(\omega_i | x) > P(\omega_j | x)$ für alle $i \not= j$
\end{itemize}

\section{Klassifizierende Diskriminantenfunktionen}

$$g_i(x) \quad , \quad i = 1, \dots, c$$
Ordne $x$ der Klasse $\omega_i$ zu, wenn $g_i(x) > g_j(x)$ für alle $j \not= i$
\begin{eqnarray*}
g_i(x) &=& P(\omega_i | x) \\ &=& \frac{p(x | \omega_i) P(\omega_i)}{\sum\limits_{j=1}^c p(x | \omega_j)P(\omega_j)} \\ g_i(x) &=& p(x | \omega_i) P(\omega_i) \\
g_i(x) &=& \log(p(x | \omega_i)) + \log(P(\omega_i))
\end{eqnarray*}

\section{Classifier Design in Practice}
\begin{itemize}
	\item Need a priori probability $P(\omega_i)$ (not too bad), class conditional PDF $p(x/\omega_i)$
	\begin{itemize}
		\item limited training data / computation
		\item class-labelling potentially costly and errorful or unknown
		\item good features not known
	\end{itemize}
	\item Parametric Solution $\to$ assume $p(x/\omega_i)$ has a particular parametric form (commonly multivariante normal)
\end{itemize}

\section{Gauss Klassifizierer (Parametric)\index{Gauss Klassifizierer}}

Eindimensionale Normaldichte: $$p(x) = \frac{1}{\sqrt{2\pi} \sigma} e^{- \frac{1}{2} \left( \frac{\myvector{x} - \myvector{\mu}}{\sigma} \right)^2} \sim N(\myvector{\mu},\sigma^2)$$
Mehrdimensionale Dichte: $$p(x) = \frac{1}{(2\pi)^{d/2}|\Sigma|^{1/2}}e^{- \frac{1}{2} ( \myvector{x} - \myvector{\mu})^t \Sigma^{-1} (\myvector{x} - \myvector{\mu}) } \sim N(\myvector{\mu},\Sigma)$$
$$g_i(x) = - \frac{1}{2} (x - \mu_i)^t \sum_i^{-1} (x - \mu_i) - \frac{d}{2} \log(2\pi) - \frac{1}{2} \log|\Sigma_i| + \log P(\omega_i)$$
Für jede Klasse $i$ berechne aus Trainigsdaten:
\begin{itemize}
\item Kovarianz-Matrix $\Sigma_i$, Mittelwertsvektor $\myvector{\mu_i}$
\end{itemize}
\textbf{Schätzung der Parameter}
\begin{itemize}
\item MLE, Maximum Likelihood Estimation:
$$\myvector{\mu} = \frac{1}{N} \sum\limits_{k=1}^N \myvector{x_k}$$ $$\Sigma = \frac{1}{N} \sum\limits_{k=1}^N (\myvector{x_k} - \myvector{\mu})(\myvector{x_k} - \myvector{\mu})^T$$
\end{itemize}

\section{Probleme beim Klassifikationsentwurf}

\begin{description}
	\item[Merkmale] Welche, wie viele, sollen welche ignoriert werden?
	\item[Fluch der Dimensionalität] Mehr Eigenschaften $\to$ schlechtere Performance (da beschränkte Trainingsdaten) $\to$ gute Wahl, Dimensionen verringern, PCA
\end{description}

\section{Principal Component Analysis (PCA)}
\begin{itemize}
	\item Assumption: Single dimensions are correlated
	\item Aim: reduce dimensions, minimum information loss
	\begin{enumerate}
		\item find axis along highest variance, rotate along this
		\item remove dimension with low variance
	\end{enumerate}
\end{itemize}

\subsection{Risiko}

\begin{itemize}
\item Entscheidungsverweigerungen in mehrdeutigen Fälle ($\to$ Bandbreite)
\item Bewerte die Kosten für jede Entscheidung (etwas Kostenaufwendiger als anders) $$\Omega = \{ \omega_1, \dots, \omega_s \} \,\, s \textrm{ Zustände der Eigenschaften}$$ $$A = \{ \alpha_1, \dots, \alpha_a \} a \textrm{ mögliche Aktionen}$$
\end{itemize}

\textbf{Verlustfunktion}
$\lambda(\alpha_i | \omega_j)$: Verlust der Aktion $\alpha_i$ beim gegebenen Zustand $\omega_j$ $$P(\omega_j | \myvector{x}) = \frac{P(\myvector{x} | \omega_j) P(\omega_j)}{P(\myvector{x})}$$
Angenommener Verlust von Aktion $\alpha_i$: $$R(\alpha_i | \myvector{x}) = \sum\limits_{j=1}^s \lambda(\alpha_i | \omega_j) P(\omega_j | \myvector{x}) \qquad \textrm{(bedingtes Risiko)}$$
Minimierung des angenommenen Verlusts indem man die Aktion $\alpha_i$ wählt, die das bedingte Risiko minimiert.
\subsubsection*{Zwei Kategorien Fall}
$$\lambda(\alpha_i | \omega_j) \triangleq \lambda_{ij}$$
\begin{eqnarray*}
R(\alpha_1 | \myvector{x}) &=& \lambda_{11} P(\omega | \myvector{x}) + \lambda_{12} P(\omega_2 | \myvector{x}) \\
R(\alpha_2 | \myvector{x}) &=& \lambda_{21} P(\omega | \myvector{x}) + \lambda_{22} P(\omega_2 | \myvector{x}) \\
\end{eqnarray*}

\begin{itemize}
\item Wähle $\omega_1$, wenn $R(\alpha_1 | \myvector{x}) < R(\alpha_2 | \myvector{x})$
\item Wähle $\omega_1$, wenn $(\lambda_{21} - \lambda_{11} P(\omega_1 | \myvector{x}) > (\lambda_{12} - \lambda_{22} P(\omega_2 | \myvector{x})$
\item Wähle $\omega_1$, wenn $(\lambda_{21} - \lambda_{11} P(\myvector{x} | \omega_1)P(\omega_1) > (\lambda_{12} - \lambda_{22} P(\myvector{x} | \omega_2) P(\omega_2)$
\item Wähle $\omega_1$, wenn $\frac{p(\myvector{x} | \omega_1)}{p(\myvector{x} | \omega_2)} > \frac{\lambda_{12} - \lambda_{22}}{\lambda_{21} - \lambda_{11}} \cdot \frac{P(\omega_2)}{P(\omega_1)}$
\end{itemize}


\section{Minimum Error Rate Classification}
\begin{itemize}
	\item Define zero-one loss function $$\lambda(\alpha_i|\omega_j)=\left\{
	\begin{array}{c}
		0 : i=j \\
		1 : i\neq j
	\end{array}\right\} i,j=1...c
	$$
	\item $R(\alpha_i|\vec{x})=1-p(\omega_i|\vec{x})$
	\item select i that maximizes posterior $p(\omega_i|\vec{x})$ $\to$ minimize risk/error
	\item decide $\omega_i$ if $p(\omega_i|\vec{x})>p(\omega_j|\vec{x})$ for all $j\neq i$
	\item non parametric better here !
\end{itemize}

\section{Parzen Fenster (Non Parametric)}

$p(x)$ direkt aus den Daten geschätzt
\begin{itemize}
\item wähle ein Fenster mit dem Volumen $V$, zähle Samples die hineinfallen\item $p(x) \thickapprox \frac{k/n}{V}$, $k = $ Anzahl, $n = $ Anzahl Samples
\end{itemize}
Probleme:
\begin{itemize}
\item Volumen zu groß $\Rightarrow$ Auflösung geht verloren
\item Volumen zu klein $\Rightarrow$ unbeständig, schlechte Abschätzung
\end{itemize}
Setze $$V_n = \frac{1}{\sqrt{n}}$$

\section{$k$-nächster Nachbar (Non Parametric)}

Setze $$k = \sqrt{n}$$ Um Sample $x$ zu klassifizieren:
\begin{itemize}
\item finde $k$ nächste Nachbarn von $x$, ordne $x$ am häufigsten vorkommende Klasse zu
\item $k$ möglichst groß für gute Abschätzung, möglichst klein damit alle Nachbarn nah beieinander
\end{itemize}
Trainingsdatenbanken müssen groß sein.

\section{Lineare Diskriminantenfunktionen (Non-Parametric, Supervised)}

\begin{itemize}
\item Diskriminantenfunktion = Distanz von Entscheidungsfläche: $$g(x) = w_0 + \sum\limits_{i=1}^n w_ix_i = \sum\limits_{i=0}^n w_ix_i, \quad x_0 = 1$$
\item 2 Kategorien $\to$ $>0 \rightarrow [1]$, $<0 \rightarrow [2]$
\end{itemize}

\section{Fisher-lineare Diskriminante}

\begin{description}
	\item[Linear Separierbar] Jede Klasse kann durch Polygon umrundet werden
\end{description}
\begin{itemize}
\item Dimensionsreduktion: mehrdimensionalen Punkten $\to$ Liniee $y = \myvector{w} \myvector{x}$
\item Die Fisher Diskriminaten ist eine Funktion, die folgendes Kriterium maximiert $$g(x) = \frac{| \tilde{m}_1 - \tilde{m_2}|}{\tilde{s}_1 + \tilde{s}_2}$$
wobei $\tilde{m}_i = \frac{1}{n} \sum\limits_{y \in Y_i} y$ : Mittel für projezierte Muster, $\tilde{s}_i^2 = \sum\limits_{y \in Y_i} (y - \tilde{m}_i)^2$ : Streuung für projezierte Muster
\item Fisher's lineare Diskriminante:
\begin{eqnarray*}
\myvector{w} &=& s_w^{-1} (\myvector{m}_1 - \myvector{m}_2) \\ s_w &=& s_1 + s_2 \\ s_i &=& \sum\limits_{x \in X_i} (\myvector{x} - \myvector{m}_i)(\myvector{x} - \myvector{m}_i)^T
\end{eqnarray*}


\end{itemize}






