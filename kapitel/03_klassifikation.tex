% !TeX root = summary.tex

\chapter{Klassifikation}

\begin{itemize}
	\item Einordnen in die Welt
	\begin{itemize}
		\item Gesellschaftlich definierte Konventionen: Buchstaben, menschliche Artifakte, Kredite
		\item Biologisch definierte Kategorien: Katze, Hund
	\end{itemize}
	\item Komplex
	\begin{itemize}
		\item Was definiert einen Stuhl? Eine Katze?
		\item Beziehungen zu Komplex, Regeln -> Lernen
		\item Nie 100\% richtig -> Wahrscheinlichkeit
	\end{itemize}
\end{itemize}

\begin{itemize}
	\item Eine einfache Form der Klassifikation ist Template Matching
	\item Ziel: Ein Muster zu erkennen das einem abgespeicherten Beispiel ähnlich ist.
	\item Maß der Übereinstimmung zwischen Muster und Schablone zentriert an (m,n) muss maximiert werden
	$M_{f,g}(m,n)=\sum_i\sum_j f(i,j)g(i-m,j-n)$
\end{itemize}

Pattern Recognition Overview
\begin{itemize}
	\item Static Patterns, no dependence on Time or Sequential Order
	\item Important Notions
	\begin{itemize}
		\item Supervised - Unsupervised
		\item Parametric - Non-Parametric
		\item Linear - Non-linear
	\end{itemize}
	\item Classical Methods
	\begin{itemize}
		\item Bayes Classifier
		\item k-nearest neighbor
	\end{itemize}
	\item Connectionist Methods
	\begin{itemize}
		\item Perceptron
		\item Multilayer Perceptrons
	\end{itemize}
\end{itemize}

\section{Schablonenanpassung\index{Schablonenanpassung} (\textsl{Template Matching})}
\begin{itemize}
\item Eine einfache Form der Klassifikation.
\item Ziel: Ein Muster zu erkennen das einem abgespeicherten Beispiel ähnlich ist.
\item Maß der Übereinstimmung ist der Absolutbetrag zwischen Muster und Schablone (zentriert an $(m,n)$): $$M_{f,g}(m,n) = \sum\limits_i \sum\limits_j | f(i,j) g(i-m,j-n)|$$
\item Abstand $E(m,n)$ ist definiert als Quadrat der Kreuzkorrelation: $$E_{f,g} (m,n) = \left( \sum\limits_i \sum\limits_j f(i,j) g(i-m,j-n) \right)^2$$
\end{itemize}

\section{Supervised -- Unsupervised Training}

\begin{description}
\item[Supervised Training] Die zu erkenndende Klasse ist für jede Auswahl in den Trainingsdaten bekannt. Benötigt a priori Wissen von nützlichen Eigenschaften und Kenntnis / Bezeichnung von jedem Trainingsmerkmal (Kosten!).
\item[Unsupervised Training] Die Klasse ist nicht bekannt und die Struktur muss automatisch herausgefunden werden.
\end{description}

\section{Parametrisch -- Nicht-parametrisch}

Parametrisch:
\begin{itemize}
\item grundlegende Aufteilungswahrscheinlichkeit annehmen
\item Parameter der Aufteilung abschätzen
\item Beispiel: "{}Gauss Klassifikation"{}
\end{itemize}
Nicht-parametrisch:
\begin{itemize}
\item keine Aufteilung annehmen
\item Fehlerwahrscheinlichkeit oder Fehlerkriterium direkt aus den Trainingsdaten berechnen
\item Beispiele: Parzen Fenster, $k$-nächster Nachbar, Perzeptron
\end{itemize}


\section{Bayes Entscheidungstheorie}

Bayes Regel\index{Bayes Regel}: $$P(\omega_j | x) = \frac{p(x | \omega_j) P(\omega_j)}{p(x)} \quad \textrm{wobei} \quad p(x) = \sum\limits_{j} p(x | \omega_j) P(\omega_j)$$
A priori Wahrscheinlichkeit: $$P(\omega_j)$$
A posteriori Wahrscheinlichkeit: $$P(\omega_j | x)$$
Klassenbedingte Wahrscheinlichkeitsdichte: $$p(x | \omega_j)$$

\section{Zwei Klassen Fall}

$$P(error | x) = \left\{ \begin{array}{cl} P(\omega_1 | x) & \textrm{wenn wir uns für } \omega_2 \textrm{ entscheiden} \\ P(\omega_2 | x) & \textrm{sonst} \end{array} \right.$$
Der Fehler ist minimiert, wenn wir uns entscheiden für:
\begin{itemize}
\item $\omega_1$ wenn $P(\omega_1 | x) > P(\omega_2 | x)$ \\ $\omega_2$ sonst
\item $\omega_1$ wenn $p(x | \omega_1)P(\omega_1) > p(x | \omega_2)P(\omega_2)$ \\ $\omega_2$ sonst
\end{itemize}
Mehr Klassen:
\begin{itemize}
\item $\omega_i$ wenn $P(\omega_i | x) > P(\omega_j | x)$ für alle $i \not= j$
\end{itemize}

\section{Klassifizierende Diskriminanzfunktionen}

$$g_i(x) \quad , \quad i = 1, \dots, c$$
Ordne $x$ der Klasse $\omega_i$ zu, wenn $g_i(x) > g_j(x)$ für alle $j \not= i$
\begin{eqnarray*}
g_i(x) &=& P(\omega_i | x) \\ &=& \frac{p(x | \omega_i) P(\omega_i)}{\sum\limits_{j=1}^c p(x | \omega_j)P(\omega_j)} \\ g_i(x) &=& p(x | \omega_i) P(\omega_i) \\
g_i(x) &=& \log(p(x | \omega_i)) + \log(P(\omega_i))
\end{eqnarray*}

\section{Classifier Design in Practice}
\begin{itemize}
	\item Need a priori probability $P(\omega_i)$ (not too bad)
	\item Need class conditional PDF $p(x/\omega_i)$
	\item Problems:
	\begin{itemize}
		\item limited training data
		\item limited computation
		\item class-labelling potentially costly and errorful
		\item classes may noat be known
		\item good features not known
	\end{itemize}
	\item Parametric Solution
	\begin{itemize}
		\item Assume that $p(x/\omega_i)$ has a particular prametric form
		\item Most common representative: multivariate normal density
	\end{itemize}
\end{itemize}

\section{Gauss Klassifizierer\index{Gauss Klassifizierer}}

Eindimensionale Normaldichte: $$p(x) = \frac{1}{\sqrt{2\pi} \sigma} e^{- \frac{1}{2} \left( \frac{\myvector{x} - \myvector{\mu}}{\sigma} \right)^2} \sim N(\myvector{\mu},\sigma^2)$$
Mehrdimensionale Dichte: $$p(x) = \frac{1}{(2\pi)^{d/2}|\Sigma|^{1/2}}e^{- \frac{1}{2} ( \myvector{x} - \myvector{\mu})^t \Sigma^{-1} (\myvector{x} - \myvector{\mu}) } \sim N(\myvector{\mu},\Sigma)$$
$$g_i(x) = - \frac{1}{2} (x - \mu_i)^t \sum_i^{-1} (x - \mu_i) - \frac{d}{2} \log(2\pi) - \frac{1}{2} \log|\Sigma_i| + \log P(\omega_i)$$
Für jede Klasse $i$ muss folgendes aus den Trainingsdaten berechnet werden:
\begin{itemize}
\item Kovarianz-Matrix $\Sigma_i$ \item Mittelwertsvektor $\myvector{\mu_i}$
\end{itemize}
\textbf{Schätzung der Parameter}
\begin{itemize}
\item MLE, Maximum Likelihood Estimation
\item Für den mehrdimensionalen Fall:
$$\myvector{\mu} = \frac{1}{N} \sum\limits_{k=1}^N \myvector{x_k}$$ $$\Sigma = \frac{1}{N} \sum\limits_{k=1}^N (\myvector{x_k} - \myvector{\mu})(\myvector{x_k} - \myvector{\mu})^T$$
\end{itemize}

\section{Probleme beim Klassifikationsentwurf}

Merkmale:
\begin{itemize}
\item Welche und wie viele Merkmale sollten gewählt werden?
\item Beliebige Merkmale?
\item Je mehr desto besser?
\item Wenn zusätzliche Merkmale nicht nützlich sind, sollen sie dann automatisch ignoriert werden?
\end{itemize}
\textbf{Der Unsegen der Dimensionalität}
\begin{itemize}
\item Allgemein gilt: das Hinzufügen von Eigenschaften verschlechtert die Performance!
\item Grund: Trainingsdaten vs. Anzahl der Parameter; beschränkte Trainingsdaten
\item Lösung: Eigenschaften sorgfältig wählen; Dimension verringern; Principle Component Analysis
\end{itemize}

\section{Principal Component Analysis (PCA)}
\begin{itemize}
	\item Assumption: Single dimensions are correlated
	\item Aim: Reduce number of dimensions with minimum loss of information
	\item Remove dimensions with low variance
\end{itemize}

\section{Risiko}

\begin{itemize}
\item Es kann zu Entscheidungsverweigerungen kommen in mehrdeutigen Fälle ($\to$ Bandbreite)
\item Bewerte die Kosten für jede Entscheidung (etwas Kostenaufwendiger als anders) $$\Omega = \{ \omega_1, \dots, \omega_s \} \,\, s \textrm{ Zustände der Eigenschaften}$$ $$A = \{ \alpha_1, \dots, \alpha_a \} a \textrm{ mögliche Aktionen}$$
\end{itemize}

\textbf{Verlustfunktion}
$\lambda(\alpha_i | \omega_j)$: Verlust der Aktion $\alpha_i$ beim gegebenen Zustand $\omega_j$ $$P(\omega_j | \myvector{x}) = \frac{P(\myvector{x} | \omega_j) P(\omega_j)}{P(\myvector{x})}$$
Angenommener Verlust von Aktion $\alpha_i$: $$R(\alpha_i | \myvector{x}) = \sum\limits_{j=1}^s \lambda(\alpha_i | \omega_j) P(\omega_j | \myvector{x}) \qquad \textrm{(bedingtes Risiko)}$$
Minimierung des angenommenen Verlusts indem man die Aktion $\alpha_i$ wählt, die das bedingte Risiko minimiert.
\subsubsection*{Zwei Kategorien Fall}
$$\lambda(\alpha_i | \omega_j) \triangleq \lambda_{ij}$$
\begin{eqnarray*}
R(\alpha_1 | \myvector{x}) &=& \lambda_{11} P(\omega | \myvector{x}) + \lambda_{12} P(\omega_2 | \myvector{x}) \\
R(\alpha_2 | \myvector{x}) &=& \lambda_{21} P(\omega | \myvector{x}) + \lambda_{22} P(\omega_2 | \myvector{x}) \\
\end{eqnarray*}

\begin{itemize}
\item Wähle $\omega_1$, wenn $R(\alpha_1 | \myvector{x}) < R(\alpha_2 | \myvector{x})$
\item Wähle $\omega_1$, wenn $(\lambda_{21} - \lambda_{11} P(\omega_1 | \myvector{x}) > (\lambda_{12} - \lambda_{22} P(\omega_2 | \myvector{x})$
\item Wähle $\omega_1$, wenn $(\lambda_{21} - \lambda_{11} P(\myvector{x} | \omega_1)P(\omega_1) > (\lambda_{12} - \lambda_{22} P(\myvector{x} | \omega_2) P(\omega_2)$
\item Wähle $\omega_1$, wenn $\frac{p(\myvector{x} | \omega_1)}{p(\myvector{x} | \omega_2)} > \frac{\lambda_{12} - \lambda_{22}}{\lambda_{21} - \lambda_{11}} \cdot \frac{P(\omega_2)}{P(\omega_1)}$
\end{itemize}


\section{Minimum Error Rate Classification}
\begin{itemize}
	\item Decision rule to minimize error rate
	\item Define zero-one loss function
	\item $\lambda(\alpha_i|\omega_j)=\left\{
	\begin{array}{c}
		0 : i=j \\
		1 : i\neq j
	\end{array}\right\} i,j=1...c
	$
	\item $R(\alpha_i|\vec{x})=\sum^{c}_{j=1}\lambda(\alpha_i|\omega_j)p(\omega_j|\vec{x})$
	\item $=\sum_{j\neq i}p(\omega_j|\vec{x})=1-p(\omega_i|\vec{x})$
	\item To minimize risk and the average probability of error, select i that maximizes posterior $p(\omega_i|\vec{x})$
	\item Decide $\omega_i$ if $p(\omega_i|\vec{x})>p(\omega_j|\vec{x})$ for all $j\neq i$
	\item Normal distribution does not model this situation well
	\item other densities may be mathematically intractable -> non-parametric techniques
\end{itemize}

\section{Parzen Fenster}

Es werden keine Annahmen über die Verteilung gemacht, stattdessen wird $p(x)$ direkt aus den Daten geschätzt.
\begin{itemize}
\item wähle ein Fenster mit dem Volumen $V$
\item zähle die Anzahl Samples, die in das Fenster fallen
\item $p(x) \thickapprox \frac{k/n}{V}$, $k = $ Anzahl, $n = $ Anzahl Samples
\end{itemize}
Probleme:
\begin{itemize}
\item Volumen zu groß \\ $\Rightarrow$ Auflösung geht verloren
\item Volumen zu klein \\ $\Rightarrow$ unbeständig, schlechte Abschätzung
\end{itemize}
Setze $$V_n = \frac{1}{\sqrt{n}}$$

\section{$k$-nächster Nachbar}

Volumen als Funktion der Daten. Verwende die $k$ nächsten Nachbarn für die Abschätzung. Setze $$k = \sqrt{n}$$ Um Sample $x$ zu klassifizieren:
\begin{itemize}
\item finde $k$ nächste Nachbarn von $x$
\item bestimme die am häufigsten vorkommende Klasse in diesen $k$ Samples
\item ordne $x$ dieser Klasse zu
\end{itemize}
Probleme: Für eine endliche Anzahl von Samples $n$ sollte $k$ möglichst
\begin{itemize}
\item groß sein für eine gute Abschätzung
\item klein sein, um zu garantieren, dass alle $k$ NAchbarn nah beieinander sind
\end{itemize}
Trainingsdatenbanken müssen groß sein.

\section{Entscheidungsfunktion $g(x)$}

\begin{eqnarray*}
g(\myvector{x}) > 0 &\Rightarrow& \textrm{Klasse A} \\ g(\myvector{x}) < 0 &\Rightarrow& \textrm{nicht Klasse A} \\ g(\myvector{x}) = 0 &\Rightarrow& \textrm{keine Entscheidung}
\end{eqnarray*}
$$g(\myvector{x}) = \sum\limits_{i=1}^n w_1x_1 + w_0 = \myvector{w}^T \myvector{x} + w_0$$
$\myvector{x} = (x_1,\dots,x_n)^T$ : Eigenschaftsvektor, $\myvector{w} = (w_1,\dots,w_n)^T$ : Gewichtungsvektor, $w_0$ : Schwellenwert


\section{Lineare Diskriminantenfunktionen}

\begin{itemize}
\item Keine Annahme über die Verteilung (Nicht-parametrisch)
\item Lineare Entscheidungsflächen
\item Start durch überwachtes Training (Klassen der Trainingsdaten gegeben)
\item Diskriminantenfunktion: $$g(x) = w_0 + \sum\limits_{i=1}^n w_ix_i = \sum\limits_{i=0}^n w_ix_i, \quad x_0 = 1$$
\item $g(x)$ ergibt die Distanz von der Entscheidungsfläche
\item Zwei Kategorien Fall:
\begin{eqnarray*}
g_1(x) > 0 &\Rightarrow& \textrm{Klasse 1} \\ g_1(x) < 0 &\Rightarrow& \textrm{Klasse 2}
\end{eqnarray*}
\end{itemize}

Hyperebe $H$: $g(\myvector{x}) = \sum\limits_{i=1}^n w_1x_1 + w_0 = \myvector{w}^T \myvector{x} + w_0 = 0$
$$\Rightarrow \myvector{x} = q \frac{\myvector{w}}{|| \myvector{w} ||} + r \frac{\myvector{w}}{|| \myvector{w} ||} + \myvector{x}_p$$
Vektor $q \frac{\myvector{w}}{|| \myvector{w} ||}$ entspricht: $g \left(q \frac{\myvector{w}}{|| \myvector{w} ||} \right) = 0 = q ||\myvector{w}|| + w_0$ $$\Rightarrow q = - \frac{w_0}{|| \myvector{w} ||}$$
Und mit $g(\myvector{x}) = \myvector{w}^T q \frac{\myvector{w}}{|| \myvector{w} ||} + \myvector{w}^T r \frac{\myvector{w}}{|| \myvector{w} ||} + \myvector{w} \myvector{x}_p + w_0 = -w_0 + r ||\myvector{w}|| + w_0$ erhalten wir $$r = \frac{g(\myvector{x})}{|| \myvector{w} ||}$$


\section{Fisher-lineare Diskriminante}

\begin{itemize}
\item Dimensionsreduktion
\item Projeziert eine Menge von mehrdimensionalen Punkten auf eine Line $y = \myvector{w} \myvector{x}$
\item Die Fisher Diskriminaten ist eine Funktion, die folgendes Kriterium maximiert $$g(x) = \frac{| \tilde{m}_1 - \tilde{m_2}|}{\tilde{s}_1 + \tilde{s}_2}$$
wobei $\tilde{m}_i = \frac{1}{n} \sum\limits_{y \in Y_i} y$ : Mittel für projezierte Muster, $\tilde{s}_i^2 = \sum\limits_{y \in Y_i} (y - \tilde{m}_i)^2$ : Streuung für projezierte Muster
\item Fisher's lineare Diskriminante:
\begin{eqnarray*}
\myvector{w} &=& s_w^{-1} (\myvector{m}_1 - \myvector{m}_2) \\ s_w &=& s_1 + s_2 \\ s_i &=& \sum\limits_{x \in X_i} (\myvector{x} - \myvector{m}_i)(\myvector{x} - \myvector{m}_i)^T
\end{eqnarray*}
\end{itemize}






