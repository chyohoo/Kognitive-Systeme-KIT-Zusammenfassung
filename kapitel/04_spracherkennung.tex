% !TeX root = summary.tex

\chapter{Spracherkennung}

\subsection{Menschliche Stimme}

Lung $\rightarrow$ Larynx $\rightarrow$ Vocal tract $\rightarrow$ Radiation $\rightarrow$ Speech! \\
excitation in Lung, Larynx \\
articulation in Larynx, Vocal tract (Motor controls) \\
Vocal tract $\rightarrow$ Transferfunction (Formant = Resonanzfrequenz dieser Funktion) \\
consonants = temporarely constrict airflow

\subsection{Menschmodell}

\mypic{10}{speech2}

\subsection{Spracherkennungssystem}

\mypic{10}{speech1}

\begin{description}
	\item[Vorverarbeitung] reduce data, noise, filter etc.\\ typically AA, ADC, Windowing, FFT, Power-Spectrum, ...
	\item[Nonlinear Alignment] for varying speaking rates during same utterance
	\item[Speech Alignment] is a relation, non bijective
	\item[Time Warp] is a relation that aligns reference and windowed input (common time axis)
	\item[Decoder] Find most likely word sequence through acoustic scores \underline{and} language model constraints :\\
	States -(Acoustic Models)-> Phonems -(Dictionary)-> Words -(Language Models)-> Sentences
\end{description}

\subsection*{Erfasste Erkennungsperformance}

Wortfehlerrate\index{Wortfehlerrate} = Levenstein Distanz

$$WER = \frac{\# Ins + \# Del + \# Sub}{N}$$

\subsection{Erkennung}

\begin{description}
\item[gegeben:] akustische Daten $A = a_1,a_2,\dots,a_k$
\item[Ziel:] Wortsequenz $W = w_1,w_2,\dots,w_n$ finden, so dass $\myprob{W}{A}$ maximiert wird.
\end{description}

$$W = \textit{argmax}_W P(W|A) = \frac{P(A|W) P(W)}{P(A)} = \textit{argmax}_W p(A|W) P(W)$$

Wiederholung: \textbf{Bayes Regel}\index{Bayes Regel} $$\myprob{W}{A} = \frac{\myprob{A}{W} \cdot P(W)}{P(A)}$$ wobei $\myprob{A}{W}$ : akustisches Modell (HMM), $P(W)$ : Sprachmodell, $P(A)$ : Konstante für einen kompletten Satz

Language models:
\begin{itemize}
	\item $P(W) = P(w_1 w_2 ...) = P(w_1) P(w_2|w_1) P(w_3|w_1w_2)...$
	\item $P(W|history)$ not possible. On the fly to expensive, or reduce history to class of history $C(\textit{history})$ (Bigram etc)
	\item deterministic: finite state grammars OR
	\item stochastic: transition probabilities (HMM)
\end{itemize}

\subsection{Hidden Markov Modelle\index{Hidden Markov Modelle} (HMM)}

Ein Hidden Markov Modell ist ein stochastisches Modell, das sich durch zwei Zufallsprozesse beschreiben lässt. \\
Der erste Zufallsprozess entspricht dabei einer Markow-Kette, die durch Zustände und Übergangs- wahrscheinlichkeiten gekennzeichnet ist. Die Zustände der Kette sind von außen jedoch nicht direkt sichtbar (sie sind verborgen). Stattdessen erzeugt ein zweiter Zufallsprozess zu jedem Zeitpunkt beobachtbare Ausgangssymbole gemäß einer zustandsabhängigen Wahrscheinlichkeitsverteilung. Die Aufgabe besteht häufig darin, aus der Sequenz der Ausgabesymbole auf die Sequenz der verborgenen Zustände zu schließen.

\subsubsection*{Elemente}

\begin{center}
\begin{tabular}{ll}
Menge an Zuständen: & $S = \{S_0, S_1, \dots, S_N\}$ \\
Übergangswahrscheinlichkeiten: & $P(q_t = S_i \, | \, q_{t-1} = S_j \} = a_{ij}$ \\
Ausgabewahrscheinlichkeitsverteilungen: & $P(y_t = O_k \, | \, q_t = S_j) = b_j(k)$ \\
(bei Zustand $j$ für Symbol $k$)
\end{tabular}
\end{center}

\subsubsection*{HMM Probleme und Lösungen}

\begin{description}
\item[Evaluation:] \textsl{Problem:} Bei einem gegebenen Modell soll die Wahrscheinlichkeit einer speziellen Ausgabesequenz bestimmt werden. \\ \textsl{Lösung:} \textbf{Forward-Algorithmus}\index{Forward-Algorithmus} und \textbf{Viterbi-Algorithmus}
\item[Dekodierung:] \textsl{Problem:} Es soll die wahrscheinlichste Sequenz der Zustände bestimmt werden, die eine vorgegebene Ausgabesequenz erzeugt hat. \\ \textsl{Lösung:} \textbf{Viterbi-Algorithmus}\index{Viterbi-Algorithmus}
\item[Training:] \textsl{Problem:} Gegeben ist nur die Ausgabesequenz. Es sollen die Parameter des HMM bestimmt werden, die am wahrscheinlichsten die Ausgabesequenz erzeugen. \\ \textsl{Lösung:} \textbf{Forward-Backward-Algorithmus}\index{Forward-Backward-Algorithmus}
\end{description}

\subsection{Evaluation}

Wahrscheinlichkeit einer Ausgabesequenz $O = O_1O_2 \dots O_T$ bei einem gegebenen Hidden Markov Modell $\lambda$ ist
\begin{eqnarray*}
\myprob{O}{\lambda} &=& \sum\limits_{\forall Q} \myprob{O \, , \, Q}{\lambda} \\ &=& \sum\limits_{\forall q_0, \dots, q_T} a_{q_0q_1} b_{q_2}(O_2) \cdots a_{q_{T-1}q_T}b_{q_T}(O_T)
\end{eqnarray*}
wobei $Q = q_0q_1\dots q_T$ eine Folge von Zuständen ist \\
\textbf{Nicht praktisch, da die Zahl der Wege in} $O(N^T)$ \textbf{liegt.} ($N$: Anzahl der Zustände im Modell, $T$: Anzahl der Ausgabesequenzen)

\subsubsection*{Der Forward Algorithmus}

$$\alpha_{t}(j) = P(O_1O_2 \dots O_t, \, q_t = S_j \, | \, \lambda)$$
rekursive Berechnung von $\alpha$:
\begin{eqnarray*}
\alpha_0(j) &=& \left\{ \begin{array}{cl} 1 & \textrm{ wenn } j \textrm{ Startzustand} \\ 0 & \textrm{ sonst} \end{array} \right. \\
\alpha_t(j) &=& \left( \sum\limits_{i=0}^N \alpha_{t-1}(i) a_{ij} \right) b_j (O_t) \quad t > 0
\end{eqnarray*}
($\myprob{O}{\lambda} = \alpha_T(S_N)$, Berechnung liegt in $O(N^2T)$)

\subsubsection*{Forward Trellis}

\mypictwo{8}{hmm_automat}{10}{forward_trellis}

\subsubsection*{Der Backward Algorithmus}

$$\beta_{t}(i) = P(O_{t+1} O_{t+2} \dots O_{T} \, , \, q_t = S_i \, | \, \lambda )$$
rekursive Berechnung von $\beta$:
\begin{eqnarray*}
\beta_0(i) &=& \left\{ \begin{array}{cl} 1 & \textrm{ wenn } i \textrm{ Endzustand} \\ 0 & \textrm{ sonst} \end{array} \right. \\
\beta_t(i) &=& \sum\limits_{j=0}^N a_{ij} b_j(O_{t+1}) \beta_{t+1} (j) \quad t < T
\end{eqnarray*}
($\myprob{O}{\lambda} = \beta_0(S_0) = \alpha_T(S_N)$, Berechnung liegt in $O(N^2T)$)

\subsubsection*{Backward Trellis}

\mypictwo{8}{hmm_automat}{10}{backward_trellis}

\subsection{Dekodierung}

\subsubsection*{Der Viterbi Algorithmus}

\begin{itemize}
\item Finde die Zustandsfolge $Q$, die $\myprob{O \, , \, Q}{\lambda}$ maximiert.
\item Verläuft ähnlich wie der Forward Algorithmus, jedoch wird das Maximum anstatt der Summe berechnet.
\end{itemize}
$$VP_t(i) = \max_{q_0, \dots, q_{t-1}} \myprob{O_1O_2 \dots O_{t} \, , \, q_t = i}{\lambda}$$
rekursive Berechnung:
\begin{eqnarray*}
VP_t(j) &=& \max_{i=0, \dots, N} VP_{t-1}(i) a_{ij}b_j(O_t) \quad t > 0 \\
P(O \, , \, Q \, | \, \lambda) &=& VP_T(S_N)
\end{eqnarray*}
Speicher jedes Maximum für die Ablaufverfolgung am Ende.

\subsubsection*{Viterbi Trellis}

\mypictwo{8}{hmm_automat}{10}{viterbi_trellis}

\subsection{Training}

\begin{itemize}
\item Trainiere Parameter vom Hidden Markov Modell
\begin{itemize}
\item $\lambda$ einstellen, um $\myprob{O}{\lambda}$ zu maximieren
\item kein effizienter Algorithmus für das globale, aber iterativ zum lokalen Optimum
\end{itemize}
\item Viterbi-Training
\begin{itemize}
\item berechne den Viterbi-Weg mittels aktuellem Modell
\item bewerte die Parameter neu, indem die Bezeichnung benutzt werden, die der Viterbi Algorithmus bestimmt hat
\end{itemize}
\item Baum-Welch\index{Baum-Welch} (Forward-Backward)
\begin{itemize}
\item berechne Wahrscheinlichkeiten mittels aktuellem Modell
\item Filtere $\lambda \to \lambda$ basierend auf den berechneten Werten
\item benutze $\alpha$ und $\beta$ vom Forward-Backward
\end{itemize}
\end{itemize}

\subsubsection*{Der Forward-Backward Algorithmus}

Wahrscheinlichkeit beim Übergang von $S_i$ nach $S_j$ zur Zeit $t$ bei gegebenem $O$:
\begin{eqnarray*}
\xi_t(i,j) &=& \myprob{q_t = S_i \, , \, q_{t+1} = S_j}{\lambda} \\ &=& \frac{\alpha_t(i) a_{ij} b_j(O_{t+1}) \beta_{t+1}(j)}{\myprob{O}{\lambda}}
\end{eqnarray*}

\subsubsection*{Baum-Welch Neubewertung}

\begin{eqnarray*}
\overline{a}_{ij} &=& \frac{\textrm{angenommene Anzahl an Übergängen von } S_i \textrm{ nach } S_j}{\textrm{angenommene Anzahl an Übergängen von } S_i} \\
&=& \frac{\sum_{t=1}^T \xi_t(i,j)}{\sum_{t=1}^T \sum_{j=0}^N \xi_t(i,j)} \\
\overline{b}_j(k) &=& \frac{\textrm{angenommene Dauer im Zustand } j \textrm{ mit Symbol } k}{\textrm{angenommene Dauer im Zustand } j} \\
&=& \frac{\sum_{t: O_t = k} \sum_{i=0}^N \xi_t(i,j)}{\sum_{t=1}^T \sum_{i=0}^N \xi_t(i,j)}
\end{eqnarray*}

\subsubsection*{Konvergenz des Forward-Backward Algorithmus}

\begin{enumerate}
\item initialisiere $\lambda = (A,B)$
\item berechne $\alpha$, $\beta$ und $\xi$
\item bewerte $\overline{\lambda} = (\overline{A},\overline{B})$ aus $\xi$
\item ersetze $\lambda$ durch $\overline{\lambda}$
\item wenn nicht konvergiert, gehe zu 2
\end{enumerate}
Es kann gezeigt werden, dass $\myprob{O}{\overline{\lambda}} > \myprob{O}{\lambda}$ gilt, wenn nicht $\lambda = \overline{\lambda}$ gilt.

\subsection{Sprachmodelle}

\subsubsection*{Basierend auf Grammatik}

\begin{itemize}
\item Bestimme Grammatik für mögliche Satzmuster
\item Vorteile: Lange Historie / Kontext; es wird keine große Textdatenbank benötigt
\item Problem: Grammatik zu schreiben ist sehr aufwendig; unflexibel: nur erstellte Muster können erkannt werden
\end{itemize}

\subsubsection*{N-Gram\index{N-Gram}}

\begin{itemize}
\item nächstes Wort wird anhand der Historie bestimmt
\item die Historie ist approximiert durch die letzten 2 oder 3 (allgemein $n$) Wörter
\item alles vor Wort $w_{i-n+1}$ wird in eine Äquivalenzklasse plaziert
\item schliesslich ist die Wahrscheinlichkeit für das nächste Wort gegeben durch
\begin{itemize}
\item Trigram: $\myprob{w_i}{w_{i-1} \, , \, w_{i-2}}$ \\ große Historie nötig, mit zeitasynchroner Suche (nur beste Vorgänger testen), außerdem: Testdatencoverage kleiner als mit Bigram
\item Bigram: $\myprob{w_i}{w_{i-1}}$ \\ einfach in HMM model einzupflegen
\item Unigram: $P(w_i)$
\end{itemize}
\item Vorteile:
\begin{itemize}
\item trainierbar auf großen Textdatenbanken, "{}milde"{} Vorhersage
\item kann direkt kombiniert werden mit einem Akustikmodell
\end{itemize}
\item Problem: benötigt eine große Textdatenbank für jedes Gebiet
\end{itemize}

Interpolation/Smoothing von NGrammen
\begin{itemize}
	\item $P(W|\textit{history}) = \frac{|history w|}{|history|}$ normal mit Trainigsdaten
	\item Wenn N Gramme nicht vorkommen ? Seltene ? $\to$ schätze seltene N Gramme mit sehr robusten N Grammen
	\item $P(w_3 | w_1 w_2) = \lambda_1 \frac{|w_1 w_2 w_3|}{|w_1 w_2|} + \lambda_2 \frac{|w_1 w_2|}{|w_1|} + \lambda_3 \frac{|w_1|}{|w_1| + |w_2| + |w_3|}$
\end{itemize}

\subsubsection*{Objektive Bewertung der Qualität von Sprachmodellen}

Genau ein Sprachmodell ist besser als eine Alternative, wenn die Wahrscheinlichkeit $\hat{P}(w_1,w_2,\dots ,w_n)$ mit dem es den Großteil eines Tests $W$ erzeugen würde, größer ist. Aber:
$$\hat{P}(w_1,w_2,\dots ,w_n) = \prod\limits_{i=1}^n Q(w_i \, | \, \Psi(w_1,\dots,w_{i-1}))$$
ein gutes Qualitätsmaß ist das LOGPROB:
$$\hat{H}(W) = \frac{1}{n} \sum\limits_{i=1}^n \log_2 Q(w_i \, | \, \Psi(w_1,\dots,w_{i-1}))$$
Wenn Wörter einheitlich durch Zufall aus einem Vokabular der Größe $V$ von "{}Sprachmechanismen"{} erzeugt wurden, dann gilt
$$Q(w_i \, | \, \Psi(w_1,\dots,w_{i-1})) = \frac{1}{V}$$ und $$2^{\hat{H}(W)} = 2^{\log V} = V$$
Also definieren wir die Perplexität\index{Perplexität} eines Sprachmodells folgendermaßen: $$PP(W) = 2^{\hat{H}(W)}$$
und interpretieren sie als "{}Abzweigungsfaktor"{} der Sprache, wenn $\Psi$ gegeben ist.

\subsubsection{Natural Language Processing}

\begin{description}
	\item[Voice Agents] intelligent, kontext / emotionsbasierte Antworten
	\item[Approach] Statistisch vs. Handdefinierte Grammatiken, $\to$ Statistik, Data is cheaper
	\item[Alignment] in verschiedenen Sprachen, Anordnung und Korrelation der Wörter
\end{description}

\begin{itemize}
	\item Interlingua \\
	Schwer, aber weniger Translator nötig, schnelle Addition von Sprachen, Kontextuell und kulturell korrekte Übersetzungen
	\item Transfer based
	\item Direct (example based or statistical)
\end{itemize}

\subsubsection{SMT Tasks}

\begin{description}
	\item[Why likelyhood?]: improve, smaller searchspace
\end{description}
\begin{itemize}
	\item Modeling: statistical models for translation equivalence like word alignment models or phrase translation
	\item Training
	\item Decoding: find best translation according to current models
\end{itemize}






