% !TeX root = summary.tex

\chapter{3D-Bildverarbeitung}



\subsection{Geometrische 3D-Transformationen}

Grundlage von Sensorik und Aktorik: Beschreibung von Objektposen (Position, Rotation)
\begin{itemize}
\item Pose des messenden Systems im Raum
\item Pose mehrerer Sensoren zueinander
\item Pose sensorisch erfasster Objekte relativ zum Sensor
\item Pose von Aktoren (Greifer, Lötlampen etc.) im Raum und relativ zum manipulierten Objekt
\item Gelenkwinkelstellungen im Roboterarm
\end{itemize}
Anforderungen:
\begin{itemize}
\item geschlossene Ausdrücke
\item Invertierbarkeit
\item Interpolation
\end{itemize}
Zwei Systeme haben sich durchgesetzt:
\begin{itemize}
\item Homogene Geometrie (für Translationen und Rotationen)
\item Quaternionendarstellung (nur für Rotationen)
\end{itemize}

\subsubsection*{Translation}\index{Translation}

Translation eines 3D-Vektors: $$\myvecthree{x_0}{y_0}{z_0} + \myvecthree{x}{y}{z} = \myvecthree{x_0 + x}{y_0 + y}{z_0 + z}$$

\subsubsection*{Rotation}\index{Rotation}

\begin{itemize}
\item o.B.d.A. auf Einheitsvektor zurückführbar (Basistransformation)
\item Konvention: Rechtskoordinatensystem
\item Rotation von $(x_0,y_0,z_0)$ um Winkel $\beta$ mit Ergebnis $(x,y,z_0)$
\end{itemize}
Aus Additionstheorem:
\begin{eqnarray*}
x &=& \cos(\alpha + \beta) = \cos(\alpha) \cos(\beta) - \sin(\alpha) \sin(\beta) \\ y &=& \cos(\alpha + \beta) = \sin(\beta) \cos(\alpha) + \cos(\beta) \sin(\alpha)
\end{eqnarray*}
und mit $(x_0,y_0) = (\cos(\alpha), \sin(\alpha))$:
$$\left( \begin{array}{c} x \\ y \end{array} \right) = \left( \begin{array}{rr} \cos(\beta) & -\sin(\beta) \\ \sin(\beta) & \cos(\beta) \end{array} \right) \left( \begin{array}{c} x_0 \\ y_0 \end{array} \right)$$
$z_0$ invariant, da Rotation um $z$!

\subsubsection*{Rotationsmatrix}\index{Rotationsmatrix}

Rotationsmatrix allgemein: $$\myvecthree{x}{y}{z} = R \myvecthree{x_0}{y_0}{z_0}$$
Rotation um $x$: $$R_x(\theta) = \myvecnine{1}{0}{0}{0}{\cos(\theta)}{- \sin(\theta)}{0}{\sin(\theta)}{\cos(\theta)}$$
Rotation um $y$: $$R_y(\theta) = \myvecnine{\cos(\theta)}{0}{\sin(\theta)}{0}{1}{0}{- \sin(\theta)}{0}{\cos(\theta)}$$
Rotation um $z$: $$R_z(\theta) = \myvecnine{\cos(\theta)}{- \sin(\theta)}{0}{\sin(\theta)}{\cos(\theta)}{0}{0}{0}{1}$$

\textbf{\textsl{Eigenschaften von Rotationsmatrizen:}}
\begin{itemize}
\item regulär, invertierbar, Determinante $=1$
\item jede beliebige Rotation im Raum kann durch drei Variablen beschrieben werden (Eulers Theorem)
\item Einzelrotationen können als eine Matrix dargestelt werden: $$R_r(\gamma) R_q(\beta) R_p(\alpha) \myvecthree{x}{y}{z} \quad \textrm{mit} \quad p,q,r \in \{x,y,z\} = R_{pqr}(\alpha, \beta, \gamma) \myvecthree{x}{y}{z}$$
\item damit reicht Angabe von $\alpha$, $\beta$, $\gamma$ zur Beschreibung der Rotation
\item das macht natürlich nur Sinn, wenn eine Konvention für die Zuordnung $p$, $q$, $r$ zu den Achsen $x$, $y$, $z$ definiert wurde
\end{itemize}
Zwei grundlegend unterschiedliche Rotationstypen:
\begin{itemize}
\item Rotation um mitgedrehte Achsen (Euler-Winkel\index{Euler-Winkel})
\item Rotation um raumfeste Achsen (Roll Pitch Yaw\index{Roll Pitch Yaw})
\end{itemize}
Konventionen zur Erstellung von Rotationsmatrizen:
\begin{itemize}
\item Standard-Beispiel für Euler-Winkel: \\ Zuerst um die $x$-Achse, dann um die mitgedrehte $y$-Achse, dann um die (zweimal) mitgedrehte $z$-Achse $$R_{X'Y'Z'}(\alpha , \beta , \gamma) = R_X(\alpha) R_Y(\beta) R_Z(\gamma)$$
\item Standard-Beispiel für raumfeste Achsen: $$R_{XYZ} = R_Z(\alpha) R_Y(\beta) R_X(\gamma)$$
\end{itemize}

\subsubsection*{Homogene 3D-Transformation}

Transformation definiert durch Rotation $R$ und Translation $t$:
$$\myvecthree{x}{y}{z} = R \myvecthree{x_0}{y_0}{z_0} + t = \myvecnine{r_{11}}{r_{12}}{r_{13}}{r_{21}}{r_{22}}{r_{23}}{r_{31}}{r_{32}}{r_{33}} \myvecthree{x_0}{y_0}{z_0} + \myvecthree{t_x}{t_y}{t_z}$$
Darstellung mit Hilfe homogener Koordinaten und einer geschlossenen Transformationsmatrix:
$$\myvecqfour{x}{y}{z}{1} = \left( \begin{array}{ccc|c} &&& \\ & R && t \\ &&& \\ \hline 0 & 0 & 0 & 1 \end{array} \right) \myvecqfour{x_0}{y_0}{z_0}{1} = \left( \begin{array}{ccc|c} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \\ \hline 0 & 0 & 0 & 1 \end{array} \right) \myvecqfour{x_0}{y_0}{z_0}{1} = A \myvecqfour{x_0}{y_0}{z_0}{1}$$

\textbf{\textsl{Probleme mit Rotationsmatrizen:}}
\begin{itemize}
\item hoch redundant
\item rechenaufwendig
\item Interpolation schwierig
\item numerisch unrobust (Singularitäten, Rundungsfehler)
\end{itemize}

\subsubsection*{Quaternionen\index{Quaternionen}}

Erweiterung der komplexen zahlen ins vierdimensionale. Definition: \\[0,1cm]
Ein Quaternion $\textbf{q}$ ist eine Zahl
\begin{eqnarray*}
\quaternion &=& (q_x,q_y,q_z,q_w)(i,j,k,1)^T \\ &=& (\quaternion_v,q_w)(i,j,k,1)^T \\ &=& iq_x + jq_y + kq_z + q_w
\end{eqnarray*}
mit
\begin{eqnarray*}
i^2 &=& j^2 = k^2 = -1 \\ ij &=& -ji = k \\ jk &=& -kj = i \\ ki &=& -ik = j
\end{eqnarray*}
$q_w$ ist der Realteil, $\quaternion_v = (q_x,q_y,q_z)$ der Imaginärteil des Quaternions. Man schreibt einfach $(q_x,q_y,q_z,q_w)$ oder $(\quaternion_v,q_w)$. \\[0,1cm]
Rechenregeln für Quaternionen:

Konjugation: \[ \overline{\quaternion} = (-\quaternion_v, q_w)\]

Norm: $$N(\quaternion) = \sqrt{\quaternion \overline{\quaternion}} = \sqrt{\overline{\quaternion}\quaternion} = \sqrt{q_x^2 + q_y^2 + q_z^2 + q_w^2}$$
Quaternionen $\quaternion$ mit $N(\quaternion) = 1$ heißen Einheitsquaternionen\index{Einheitsquaternionen}. \\ Multiplikative Identität: $$I = (0,1)$$ Multiplikative Inverse: $$\quaternion^{-1} = \frac{\overline{\quaternion}}{N^2(\quaternion)}$$
Rotation mit Quaternionen:
\begin{itemize}
\item Einheitsquaternion $\quaternion$ ist definiert durch Rotationsachse $u$ mit $|u| = 1$ und Winkel $\theta$: $$\quaternion = \left( \cos \frac{\theta}{2} \, , \, u \sin \frac{\theta}{2} \right)$$
\item Quaternion \textbf{a} ist definiert durch zu rotierenden Vektor $v$: $$\textbf{a} = (v \, , \, 0)$$
\item Das Produkt $\textbf{qa}\overline{\textbf{q}}$ rotiert $v$ um die Achse $u$ mit dem Winkel $\theta$.
\end{itemize}
Interpolation zwischen zwei Quaternionen:
\begin{itemize}
\item Sphärische Lineare Interpolation (SLERP)
\item Berechnet für $t \in [0,1]$ die kürzeste Verbindung auf der vierdimensionalen Einheitssphäre zwischen $q$ und $r$.
\item Analytisch: $$SLERP(q,r,t) = q(rq^{-1})^t$$
\item Numerisch: $$SLERP(q,r,t) = q \frac{\sin((1-t)\theta)}{\sin(\theta)} + r \frac{\sin(t \theta)}{\sin(\theta)}$$ mit Winkel $\theta$ zwischen $r$ und $q$.
\end{itemize}
Quaternion $\to$ Rotationsmatrix: $$\quaternion = (q_x,q_y,q_z,q_w) \Rightarrow M_q = \myvecnine{1-2(q_y^2 + q_z^2)}{2(q_xq_y - q_wq_z)}{2(q_xq_z + q_wq_y}{2(q_xq_y + q_wq_z}{1-2(q_x^2 + q_z^2)}{2(q_yq_z - q_wq_x)}{2(q_xq_z - q_wq_y}{2(q_yq_z + q_wq_x}{1-2(q_x^2 + q_y^2)}$$
Rotationsmatrix $\to$ Quaternion:
\begin{eqnarray*}
q_w &=& \frac{1}{2} \sqrt{1 + \sum\limits_{i=1}^3 m_{ii}} \\ q_x &=& \frac{(m_{32} - m_{23})}{4q_w} \\ q_y &=& \frac{m_{13} - m_{31})}{4q_w} \\ q_z &=& \frac{(m_{21} - m_{12})}{4q_w}
\end{eqnarray*}

\textbf{\textsl{Vor- und Nachteile}} \\
Vorteile:
\begin{itemize}
\item Rotation direkt um gewünschte Drehachse
\item Interpolation möglich
\item weniger Rechenaufwand
\item keine Redundanz $\Rightarrow$ numerisch stabiler, weniger Gefahr für Singularitäten
\end{itemize}
Nachteil:
\begin{itemize}
\item nur Rotation berechenbar $\Rightarrow$ Kombination mit Matrizen nötig $\Rightarrow$ Rechenaufwand für Umwandlungen
\end{itemize}

\subsubsection*{Realistischeres Kameramodell}

Lochkameramodell vereinfacht die realen Verhältnisse stark. Deshalb werden in der Praxis Erweiterungen des Lochkameramodells verwendet. Zunächst einige Definitionen:
\begin{description}
\item[Optische Achse:] Gerade durch das Projektionszentrum, senkrecht zur Bildebene
\item[Bildhauptpunkt $C(c_x \, , \, c_y)$:] Schnittpunkt der optischen Achse mit der Bildebene
\end{description}
Koordinatensysteme:
\begin{description}
\item[Bildkoordinatensystem:\index{Bildkoordinatensystem}] 2D-Koordinatensystem, Einheit [Pixel], Vereinbarung für die Vorlesung (gilt für die meisten Kameratreiber): Ursprung in der linken oberen Ecke des Bildes, $u$-Achse zeigt nach rechts, $v$-Achse zeigt nach unten.
\item[Kamerakoordinatensystem:\index{Kamerakoordinatensystem}] 3D-Koordinatensystem, Einheit [mm], Ursprung liegt im Projektionszentrum, Achsen parallel zu den Achsen des Bildkoordinatensystems, d.h. $x$-Achse nach rechts, $y$-Achse nach unten und die $z$-Achse gemäß der Dreifingerregel für ein rechtshändiges Koordinatensystem nach vorne.
\item[Weltkoordinatensystem:\index{Weltkoordinatensystem}] 3D-Koordinatensystem, Einheit [mm], Basiskoordinatensystem, das beliebig im Raum liegen kann.
\end{description}
Begriffe:
\begin{description}
\item[Intrinsische Kameraparameter:] Brennweite, Bildhauptpunkt, Parameter für die Beschreibung radialer/tangentialer Linsenverzerrung; definieren die nicht (eindeutig) umkehrbare Abbildung vom Kamerakoordinatensystem in das Bildkoordinatensystem.
\item[Extrinsische Kameraparameter:] Definieren die Transformation vom Kamerakoordinatensystem in das Weltkoordinatensystem, im Allgemeinen durch eine Rotation $R$ und eine Translation $t$.
\end{description}
Vereinfachungen des Lochkameramodells:
\begin{itemize}
\item Ursprung des Bildkoordinatensystems ist identisch mit dem Bildhauptpunkt
\item Pixel werden als quadratisch angenommen
\item keinerlei Modellierung der Linsenverzerrung
\item es existiert kein Weltkoordinatensystem bzw. es ist identisch mit dem Kamerakoordinatensystem, d.h. es werden keine extrinsischen Kameraparameter modelliert
\end{itemize}
Brennweite:
\begin{itemize}
\item In der Praxis wird die Umrechnung von [mm] nach [Pixel] in den/die Parameter für Brennweite mit aufgenommen.
\item Da Pixel nicht mehr als quadratisch sondern als rechteckig angenommen werden, gibt es deshalb für jede Richtung einen Parameter, also: $f_x$, $f_y$.
\item Die Parameter $f_x$, $f_y$ sind dann das Produkt aus der tatsächlichen Brennweite mit Einheit [mm] und dem jeweiligen Umrechnungsfaktor mit Einheit [Pixel/mm].
\item Die Einheit für die Parameter $f_x$, $f_y$ ist somit [Pixel].
\end{itemize}
Die Abbildung vom Kamerakoordinatensystem in das Bildkoordinatensystem, ausschließlich mit den intrinsischen Parametern, ist dann definiert durch: $$\myvectwo{u}{v} = \myvectwo{c_x}{c_y} + \frac{1}{Z} \cdot \myvectwo{f_x \cdot X}{f_y \cdot Y}$$ oder als Matrixmultiplikation mit Kalibriermatrix $K$ auf homogenen Koordinaten: $$\myvecthree{u \cdot w}{v \cdot w}{w} = K \cdot \myvecthree{X}{Y}{Z} \qquad K = \myvecnine{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}$$
Extrinsische Kamerakalibrierung:
\begin{itemize}
\item Ist definiert durch eine Koordinatentransformation bestehend aus Rotation und Translation.
\item Koordinatentransformation vom Weltkoordinatensystem in das Kamerakoordinatensystem: $$x_c = Rx_w + t$$
Koordinatentransformation vom Kamerakoordinatensystem in das Weltkoordinatensystem: $$x_w = R^T x_c - R^T t$$
\item $3 \times 4$ Gesamt-Projektionsmatrix $P$ (intrinsisch und extrinsisch) auf homogenen Koordinaten: $$\myvecthree{u \cdot w}{v \cdot w}{w} = P \cdot \myvecqfour{X}{Y}{Z}{1} \qquad P = (K \, R \, | \, K \, t)$$
\end{itemize}

\subsubsection*{Kamerakalibrierung\index{Kamerakalibrierung}}

Die Kalibrierung einer Kamera bedeutet die Bestimmung ihrer Parameter bezüglich eines gewählten Kameramodells. Die Bestimmung der intrinsischen Parameter ist unabhängig vom Aufbau; solange Zoom und Fokus der Kamera gleich bleiben, verändern sich diese Parameter nicht. Die Bestimmung der extrinsischen Parameter ist abhängig von der Wahl des Weltkoordinatensystems und ändert sich je nach Aufbau. \\
Ist die Kamera kalibriert, dann liegt die Abbildungsfunktion $f$ vor, die einen Punkt vom Weltkoordinatensystem eindeutig in das Bildkoordinatensystem abbildet: $$f \, : \, R^3 \to R^2$$
$f$ ist definiert durch die Projektionsmatrix $P$ und anschließender Transformation der homogenen Koordinaten durch Division durch $w$. Die Inverse Abbildung bildet einen Punkt im Bildkoordinatensystem auf eine Gerade im Weltkoordinatensystem ab, die durch das Projektionszentrum verläuft. \\[0,1cm]
Verfahren zur Kamerakalibrierung:
\begin{itemize}
\item Direkte Lineare Transformation (DLT)
\item Erweiterungen der DLT, welche Linsenverzerrung modellieren
\end{itemize}
gesucht: $3 \times 4$-Matrix, hat also 12 Unbekannte; Verfahren Testfeldkalibrierung:\index{Testfeldkalibrierung}
\begin{itemize}
\item Bestimmung einer Menge von Punktkorrespondenzen: 3D-Punkt in einem gewählten Weltkoordinatensystem und 2D-Punkt im Bildkoordinatensystem
\item 3D-Punkte sind durch Verwendung eines geeigneten Kalibrierobjekts oder -musters a-priori bekannt
\item 2D-Punkte werden durch Methoden der Bildverarbeitung berechnet
\end{itemize}
benötigt: 6 bekannte Objektpunkte, da jede Punktkorrespondenz zwei Gleichungen liefert \\
Bedingung: 3D-Punkte dürfen nicht koplanar liegen, d.h. sie müssen einen dreidimensionalen Raum aufspannen \\
Möglichkeiten:
\begin{itemize}
\item Verwendung eines 2D-Musters, das in mindestens zwei verschiedenen Tiefen präsentiert wird.
\item Verwendung eines geeigneten 3D-Kalibrierobjekts.
\end{itemize}

\subsubsection*{Direkte Lineare Transformation}

Ein Standard-Verfahren für die Berechnung der Projektionsmatrix $P$ ist die Direkte Lineare Transformation (DLT)
$$\myvecthree{u \cdot w}{v \cdot w}{w} = P \cdot \myvecqfour{X}{Y}{Z}{1} \qquad P = (K \, R \, | \, K \, t) = \left( \begin{array}{cccc} p_1 & p_2 & p_3 & p_4 \\ p_5 & p_6 & p_7 & p_8 \\ p_9 & p_{10} & p_{11} & p_{12} \end{array} \right)$$
\begin{eqnarray*}
\Rightarrow u &=& \frac{p_1X + p_2Y + p_3Z + p_4}{p_9X + p_{10}Y + p_{11}Z + p_{12}} \\
v &=& \frac{p_5X + p_6Y + p_7Z + p_8}{p_9X + p_{10}Y + p_{11}Z + p_{12}}
\end{eqnarray*}
o.B.d.A. kann ein Parameter normiert werden. Üblicherweise wird $p_{12} = 1$ gewählt.
\begin{eqnarray*}
p_1X + p_2Y + p_3Z + p_4 &=& up_9X + up_{10}Y + up_{11}Z + u \\
p_5X + p_6Y + p_7Z + p_8 &=& vp_9X + vp_{10}Y + vp_{11}Z + v
\end{eqnarray*}
Formuliert als überbestimmtes LGS $Ax = b$ mit $n \geq 6$ Punktkorrespondenzen, das beispielsweise mit Hilfe der Normalengleichung gelöst werden kann:
$$A = \left( \begin{array}{ccccccccccc} X_1 & Y_1 & Z_1 & 1 & 0 & 0 & 0 & 0 & -u_1X_1 & -u_1Y_1 & -u_1Z_1 \\ 0 & 0 & 0 & 0 & X_1 & Y_1 & Z_1 & 1 & -v_1X_1 & -v_1Y_1 & -v_1Z_1 \\ \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots \\ X_n & Y_n & Z_n & 1 & 0 & 0 & 0 & 0 & -u_nX_n & -u_nY_n & -u_nZ_n \\ 0 & 0 & 0 & 0 & X_n & Y_n & Z_n & 1 & -v_nX_n & -v_nY_n & -v_nZ_n \end{array} \right) \quad x = \myvecthree{p_1}{\vdots}{p_{11}} \quad b = \left( \begin{array}{c} u_1 \\ v_1 \\ \vdots \\ u_n \\ v_n \end{array} \right)$$

\subsection{3D Bildverarbeitung}

\subsubsection*{Stereokonstruktion}

gegeben:
\begin{itemize}
\item zwei Kameras (durch ihre Zentren $C$ und $C'$) mit Projektionsmatrizen $P$ und $P'$
\item zwei Abbilder $x$ und $x'$ des Punktes $X$
\item dann kann $X$ rekonstruiert werden
\end{itemize}
Triangulation zwischen linker und rechter Kamera möglich durch Kenntnis der Kameraparameter. Eine Möglichkeit zur Berechnung von 3D-Punkten aus Bildpunkt-Korrespndenzen $x$, $x'$:
\begin{itemize}
\item Aufstellen der beiden Geraden $g$, $g'$ der möglichen Punkte zu $x$, $x'$ im Weltkoordinatensystem mit Hilfe der Projektionsmatrizen $P$, $P'$:
\begin{eqnarray*}
g \, : \, x &=& a + r \cdot u \\ g' \, : \, x &=& b + s \cdot v
\end{eqnarray*}
\item Berechnung des optimalen "{}Schnittpunktes"{} $S$ durch Lösung des überbestimmten LGS $Ax = c$ mit: $$A = \left( \begin{array}{cc} u_1 & -v_1 \\ u_2 & -v_2 \\ u_3 & -v_3 \end{array} \right) \quad , \quad x = \myvectwo{r'}{s'} \quad , \quad c = b-a \qquad s = \frac{a + r' \cdot u + b + s' \cdot v}{2}$$
\end{itemize}
3D-Tracking von Kopf und Händen auf der Basis von Hautfarbe, Region-Growing und Stereokalibrierung.

\subsubsection*{Epipolargeometrie\index{Epipolargeometrie}}

\mypic{8}{kameramodell}

Zusammenhang zwischen zwei Kameras ist gegeben durch die Epipolargeometrie. Die Schnittpunkte $e$ und $e'$ der Geraden durch die Projektionszentren mit den Bildebenen nennt man Epipole\index{Epipol}.
\begin{description}
\item[Epipolarebene $\pi(X)$:] Ebene, die durch $C$, $C'$ und Szenenpunkt $X$ aufgespannt wird.
\item[Epipolarlinie $l'(x)$:] Schnittgerade von $\pi(X)$ mit Bildebene.
\end{description}
Alle Punkte $X$, die auf $x$ in Kamera 1 abgebildet werden, werden auf einen Punkt der Linie $l'(x)$ in Kamera 2 abgebildet. Alle Epipolarlinien eines Kamerasystems schneiden sich in den Epipolen $e$ und $e'$. \\
\textsl{Nutzen:} Einschränkung des Korrespondenzproblems von zwei Dimensionen auf eine Dimension, da nach entsprechenden Merkmalen nur noch entlang der Epipolarlinie gesucht werden muss:
\begin{itemize}
\item höhere Robustheit (weniger falsche Korrespondenzen)
\item höhere Effizienz
\end{itemize}

\subsubsection*{Fundamentalmatrix\index{Fundamentalmatrix}}

Mathematische Beschreibung der Epipolargeometrie erfolgt durch die Fundamentalmatrix. Eigenschaften der Fundamentalmatrix $F$:
\begin{itemize}
\item $3 \times 3$-Matrix
\item Rang 2
\item für alle Korrespondenzen $x$, $x'$ gilt: $$x'^TFx = 0$$ $x$ und $x'$ sind Bildpunkte in homogenen Koordinaten mit $w = 1$
\end{itemize}
Mit der Fundamentalmatrix lassen sich die Epipolarlinien berechnen: $$l = F^Tx' \qquad \textrm{und} \qquad l' = Fx$$ Für die Epipole gilt: $$Fe = 0 \qquad \textrm{und} \qquad F^Te' = 0$$ Hinweis: $l$ (bzw. $l'$) definieren eine 2D-Gerade wie folgt: \\ $lx = 0$ für alle Bildpunkte $x$ (in homogenen Koordinaten mit $w = 1$), die auf dieser Geraden liegen. \\
Die Fundamentalmatrix lässt sich auf mehrere Arten berechnen:
\begin{itemize}
\item über Bildpunkt-Korrespondenzen in der linken und rechten Kamera
\item bei bekannter intrinsischer und extrinsischer Kalibrierung der Kameras direkt über die Kalibriermatrizen $K$, $K'$ und der Essentialmatrix $E$, die durch die extrinsischen Parameter definiert ist
\end{itemize}
\textbf{\textsl{Berechnung über Bildpunkt-Korrespondenzen:}}
$$x'^T Fx = 0 \quad , \quad x' = (x',y',z') \quad , \quad x = (x,y,z)$$
\begin{eqnarray*}
\Rightarrow \qquad \qquad x'x f_{11} + x'y f_{12} + x'f_{13} && \\ + \,\, y'x'f_{21} + y'yf_{22} + y'f_{23} && \\ + \,\, xf_{31} + yf_{32} + f_{33} &=& 0
\end{eqnarray*}
Für $n \geq 7$ Korrespondenzen $x$, $x'$:
$$\underbrace{\left( \begin{array}{ccccccccc} x_1'x_1 & x_1'y_1 & x_1' & y_1'x_1 & y_1'y_1 & y_1' & x_1 & y_1 & 1 \\ \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots \\ x_n'x_n & x_n'y_n & x_n' & y_n'x_n & y_n'y_n & y_n' & x_n & y_n & 1 \end{array} \right)}_{A} \underbrace{\myvecqfour{f_{11}}{f_{12}}{\vdots}{f_{33}}}_{f} = 0$$
$Af=0$ lösen z.B. mit Singulärwertzerlegung (SVD) \\
\textbf{\textsl{Berechnung über Essentialmatrix:}} \\
Essentialmatrix lässt sich durch die extrinsischen Parameter berechnen: \\
gegeben:
\begin{itemize}
\item Kamera 1 mit $(I \, | \, 0)$ als Transformation (Identität)
\item Kamera 2 mit $(R \, | \, t)$ als Transformation
\end{itemize}
Essentialmatrix $E$ lässt sich berechnen zu: $$E = [t]_xR = \myvecnineright{0}{-t_3}{t_2}{t_3}{0}{-t_1}{-t_2}{t_1}{0}$$ Für die Epipole gilt: $$e = KR^Tt \quad \textrm{und} \quad e' = K't$$
Hat man die Essentialmatrix (z.B.über die extrinsischen Parameter) berechnet und die intrinsischen Parameter, d.h. Kalibriermatrizen $K$, $K'$, so lässt sich die Fundamentalmatrix berechnen zu: $$F = K'^{-T}EK^{-1}$$
Hat man umgekehrt die Fundamentalmatrix bestimmt (z.B. über Bildpunkt-Korrespondenzen) und die intrinsischen Parameter, d.h. die Kalibriermatrizen $K$, $K'$, so lässt sich die Essentialmatrix berechnen zu: $$E = K'^TFK$$

\subsubsection*{Stereo-Sehen}

Weiter Eigenschaften der Fundamentalmatrix:
\begin{itemize}
\item Mit ihr lassen sich die Eingabebilder rektifizieren.
\begin{itemize}
\item Nach Rektifizierung verlaufen alle Epipolarlinien horizontal mit derselben $v$-Koordinate wie der Bildpunkt im anderen Kamerabild.
\item Nach Korrespondenzen muss nur noch horizontal (in eine Richtung) gesucht werden.
\end{itemize}
\item Mit Hilfe der Essentialmatrix lassen sich die Projektionsmatrizen bis auf Skalierung genau rekonstruieren, mit Hilfe der Fundametalmatrix bis auf Skalierung und Projektion genau.
\end{itemize}
Rektifizierte Bilder haben den Vorteil, dass sich optimierte Korrelations-Algorithmen für die Lösung des Korrespondenzproblems verwenden lassen. $\Rightarrow$ Laufzeit unabhängig von der Fenstergröße \\
Nachteile:
\begin{itemize}
\item Interpolation notwendig für die Berechnung der rektifizierten Bilder $\Rightarrow$ Qualitätsverlust
\item Bilder je nach Aufbau stark verzerrt
\end{itemize}
Nach Lösung des Korrespondenzproblems können
\begin{itemize}
\item Punktwolken berechnet werden durch Triangulation, wie zuvor erläutert
\item Tiefenbilder erzeugt werden durch Eintrag der Disparitäten (Differenz der $u$-Koordinaten für gefundene Korrespondenzen in den rektifizierten Bildern) in ein Graustufenbild: \\ $\Rightarrow$ Je höher der Grauwert, desto näher befindet sich der entsprechende 3D-Punkt zur Kamera
\end{itemize}




































