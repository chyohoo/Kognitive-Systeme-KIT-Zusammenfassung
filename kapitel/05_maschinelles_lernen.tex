% !TeX root = summary.tex

\chapter{Maschinelles Lernen}

\section{Unsupervised Learning}

\begin{itemize}
\item Datensammlung und Bezeichnung kostenaufwendig und zeitverschwenderisch.
\item Die Charaktermerkmale der Muster können sich ändern im Verlauf der Zeit.
\item Vielleicht hat man keine Einsicht in die Struktur der Daten.
\end{itemize}
$\Rightarrow$ Die Klassen sind \underline{nicht} bekannt.

\subsection{Mischdichten}

\begin{itemize}
\item $c$ Klassen, a priori Wahrscheinlichkeit $P(\omega_j)$
\item Annahme: Die Formen der Klassenbedingten PDF $P(X | \omega_j , \theta_j)$ sind bekannt
\item Unbekannter Parametervektor $\theta_1, \dots, \theta_c$
\end{itemize}
Problem: Mathe... also:
\begin{itemize}
\item Wähle initiale Mittel $\mu_1, \dots, \mu_c$
\item Klassifiziere $n$ Muster zu dem Mittel, das am nähesten liegt.
\item Berechne die Mittel neu aus den Mustern der Klasse, bis sich Mittel nicht mehr ändern
\end{itemize}
Isodaten, Probleme:
\begin{itemize}
\item initialen Mittel $\mu$, Anzahl Klassen??, Verteilungsannahmen?
\end{itemize}

\subsection{Clustering}

\begin{itemize}
\item Ähnlichkeit, Kriteriumsfunktion
\item Clusterqualität: Muster in selber Klassen machen Kriteriumsfunktion extremer
\item Beispiel: Summe des Fehlerkriteriums: $$J = \sum\limits_{i=1}^c \sum\limits_{\max} || x - m_i ||^2$$
\end{itemize}

\subsection{Hierarchisches Clustering}

\begin{itemize}
\item $c$ und Mittel müssen nicht erraten werden
\begin{algorithmic}
	\State Init $c := n$
	\While{$c >= C_{stop}$}
		\State Find nearest pairs of destinct cluster $X_i$ and $X_j$
		\State Merge them, $c--$
	\EndWhile
\end{algorithmic}
\end{itemize}


\section{Neuronale Netze}

aka. MLP, Parallel Distribution Processing, Connectionist Model...

\begin{itemize}
\item Massiver Parallelismus, fehlerhafte Eingaben
\item Viele einfache Prozesseinheiten, viele Verbindungen
\item Einheitlichkeit ($\to$ Sensorverschmelzung)
\item Nichtlineare Klassifizierer / Abbilder ($\to$ gute Performance, lernfähig)
\end{itemize}

\subsection{Entscheidungsfunktion $g(x)$}

\begin{eqnarray*}
g(\myvector{x}) > 0 &\Rightarrow& \textrm{Klasse A} \\ g(\myvector{x}) < 0 &\Rightarrow& \textrm{nicht Klasse A} \\ g(\myvector{x}) = 0 &\Rightarrow& \textrm{keine Entscheidung}
\end{eqnarray*}
$$g(\myvector{x}) = \sum\limits_{i=1}^n w_1x_1 + w_0 = \myvector{w}^T \myvector{x} + w_0$$
$\myvector{x} = (x_1,\dots,x_n)^T$ : Eigenschaftsvektor, $\myvector{w} = (w_1,\dots,w_n)^T$ : Gewichtungsvektor, $w_0$ : Schwellenwert

\subsection{Das Perzeptron}

\mypic{10}{perzeptron}

\subsubsection{Aktivierungsfunktion}

\begin{itemize}
	\item Hard Limiter (0 bis Schwellwert, dann 1)
	\item Threshold Logic (ab Schwellwert linear bis 1)
	\item Sigmoid ($\frac{1}{1 + e^{-x_j}}$)
\end{itemize}

\subsubsection*{Lineares Entscheidungselement}

$$g(x) = \sum\limits_{i=0}^n w_i x_i \quad x_0 = 1 \qquad \to \qquad \textrm{finde } w$$
Alle Vektoren $x_i$ sind korrekt benannt
\begin{eqnarray*}
\myvector{w}\myvector{x}_i > 0 && \textrm{ wenn } x_i \textrm{ zu } \omega_1 \textrm{ gehört} \\ \myvector{w}\myvector{x}_i < 0 && \textrm{ wenn } x_i \textrm{ zu } \omega_2 \textrm{ gehört}
\end{eqnarray*}
Oder setze alle Muster, die zu $\omega_2$ gehören auf ihr negatives ($-\myvector{x}_i$). Dann sind alle Vektoren korrekt klassifiziert, wenn $\myvector{w} \myvector{x}_i > 0$ für alle $i$.
\subsubsection*{Kriteriumsfunktion vom Perzeptron}
$$J_P(\myvector{w}) = \sum\limits_{x \in X} (- \myvector{w} \myvector{x})$$
$X$ ist die Menge der falsch klassifizierten Merkmale. Da $\myvector{w} \myvector{x}$ die Negation für falsch klassifizierte Merkmale ist, ist $J_P(\myvector{w})$ positiv. Sobald $J_P = 0$ gilt, ist ein Lösungsvektor gefunden. \\ $J_P$ ist proportional zu der Summe der Distanzen der falsch klassifizierten Mustern zur Entscheidungsgrenze.
$$\nabla J_P = \sum\limits_{x \in X} (- \myvector{x}) \qquad \myvector{w}_{k+1} = \myvector{w}_k + \zeta_k \sum\limits_{x \in X} \myvector{x}$$

\subsubsection*{Lernen des Perzeptron}

Fragen:
\begin{itemize}
\item Wie soll man die Lernrate/initialen Gewichte setzen ?
\end{itemize}
Probleme:
\begin{itemize}
\item Untrennbare (dann minimiere MSE $J_s(\myvector{w}) = \sum\limits_{x \in X} (\myvector{w}^t x - b)^2$) =  / nicht linear trennbare Daten, schwere Entscheidungsfläche
\end{itemize}

$\to$ TROTZDEM nur lineare Separierung ( XOR-Problem )

\subsubsection*{Variationen}

Entspannungsprozedur: $$J_q(\myvector{w}) = \sum\limits_{x \in X} (\myvector{w}^t \myvector{y})^2$$
Der Gradient ist kontinuierlich $\to$ gleichmäßige Fläche. Manchmal geht es gegen Null!
$$J_q(\myvector{w}) = \frac{1}{2} \sum\limits_{x \in X} \frac{(\myvector{x}^t\myvector{w} - b)^2}{||x||^2} \textrm{ begrenzt durch } b$$

\subsection{Multi Level Perceptron (MLP)}

Input $\to$ Hidden $\to$ Output

\begin{description}
	\item[Error] $E = \frac{1}{2} \sum_j(y_j - d_j)^2$
	\item[WeightedSum] $x_j = \sum_i y_i w_{ij}$
	\item[sigmoidSum] $y_j = \frac{1}{1 + e^{-x_j}}$
\end{description}

\begin{itemize}
	\item Compute error on node $g_j$ : $$\frac{\partial E}{\partial g_j} = \sum_i \sigma'(h_i) v_{ij} \frac{\partial E}{\partial h_i}$$
	\item Compute error on weight $u_{jk}$ : $$\frac{\partial E}{\partial u_{jk}} = \frac{\partial E}{\partial g_j} \sigma'(g_j)f_k$$
	\item derivate sigmoid $\sigma(x)$ : $$\sigma'(x) = \sigma(x) (1 - \sigma(x))$$
\end{itemize}

\subsubsection{Approach}

Random weights !
\begin{enumerate}
	\item Apply input, get output
	\item compare output to desired output, backpropagate $\frac{\partial E}{\partial w_{ij}}$
	\item adjust weights slightly
\end{enumerate}

\subsubsection{Weightupdate}

$\Delta w_{ij}(t) = -\epsilon \frac{\partial E}{\partial w_{ij}} (t) + \alpha \Delta w_{ij} (t - 1)$

\begin{description}
	\item[Momentum] $\alpha \in [0, 1]$ Letze Änderungsstärke
	\item[Stepsize] $\epsilon \in [0, 1]$ Änderungsrate
\end{description}

\subsubsection{Statistische Interpretation MLP}

$\rightarrow$ A posteriori Wahrscheinlichkeit $P(w|x)$
iff. 
\begin{itemize}
	\item objektive Funktion (MSE...)
	\item große Trainingsdatenbank
\end{itemize}

\subsubsection{Design Problems}

Local Minima, Speed of Learning, Architecture, Scaling...\\
How to consider context, shift invariantly, process pattern sequence

\subsubsection{Time Delayed Neural Networks}

\begin{itemize}
	\item MLN, nonlinear, Time delay at input + all layers
	\item \textbf{Shift Invariant Learning} : detect patterns independent of location in time ( through weight sharing )
\end{itemize}