% !TeX root = summary.tex

\newcommand{\myfbox}[1]{
\begin{tabular}{|l|}
\hline #1 \\ \hline
\end{tabular}
}

\chapter{Bildverarbeitung}

\section{Bildgenerierung}

\begin{itemize}
\item Heute werden oftmals Digitalkameras verwendet (meist CCD, aber auch CMOS)
\item Anschluss erfolgt über: Firewire (IEEE1394), USB, Camera Link, \dots
\item Kameras liefern direkt digitalisierte Bilddaten
\begin{itemize}
\item Format je nach Kamera und Modus unterschiedlich
\item Bei S/W-Kameras meist 8bit Graustufen
\item Bei Farbkameras entweder als bayer-Pattern oder meist bereits konvertiert als RGB24, YUV422, etc.
\end{itemize}
\item Kameras unterscheiden sich durch:
\begin{itemize}
\item Bildqualität (Qualität CCD-Chip, aber auch grundlegend: Auswahl Linse/Objektiv)
\item Graustufen- oder Farbkamera
\item Auflösung
\item Bei Farbkameras: Welche Farbkodierungen sind verfügbar? (8bit, 16bit, 24bit)
\item Wichtig je nach Anwendung: Welche maximale Framerate ist bei welchem Bildformat noch verfügbar? (z.B. 15/30/60/120/200 Hz)
\end{itemize}
\end{itemize}

\section{Bildrepräsentation}

\textbf{Monochrombild:} Diskrete Funktion
\begin{eqnarray*}
Img \, : \, [0 \dots n-1] \times [0 \dots m-1] &\to& [0 \dots q] \\ (u,v) &\mapsto& Img(u,v)
\end{eqnarray*}
Üblich: $q = 255$; $n = 640$, $m=480$ (VGA) oder $n = 768$, $m=576$ (PAL) \\[0,1cm]
\subsection{Farbbild}
\begin{itemize}
\item Viele verschiedene Farbmodelle für unterschiedliche Anwendungen
\item Klassifikation nach erreichbarem Farbraum
\item
\begin{itemize}
\item S/W, Grauwertstufen
\item RGB-Modell: speziell für Monitore (Phosphor-Kristalle), sehr üblich $$Img(u,v) \in \mathbb{R}^3 = (r,g,b)^T$$
\item HSI (Hue, Saturation, Intensity): speziell für Farbsegmentierung
\item CIE: physikalisch (Wellenlänge)
\item CMYK- Modell: Farbdrucker (subtraktive Farbmischung)
\item YIQ: Fernsehmodell
\end{itemize}
\end{itemize}

\subsubsection{RGB-Modell}\index{RGB-Modell}

\begin{eqnarray*}
Img \, : \, [0 \dots n-1] \times [0 \dots m-1] &\to& [0 \dots R] \times [0 \dots G] \times [0 \dots B] \\ (u,v) &\mapsto& Img(u,v) = (r,g,b)
\end{eqnarray*}
\begin{itemize}
\item additive Farbmischung
\item drei Farbwerte: Rot, Grün, Blau \\ oft: $256 \times 256 \times 256$ Nuancen \\ ($R=G=B=255$, 8Bit, "{}RGB24"{}) = 16,8 Mio. Farben
\item oft verwendet von Kamera-Treibern
\end{itemize}

\subsubsection{HSI-/HSV-Modell\index{HSI-/HSV-Modell}}

\begin{itemize}
\item Hue (Farbnuancen), Saturation (Sättigung), Intensity/Value (Helligkeit)
\item trennt Helligkeit vom Farbwert $\Rightarrow$ unempfindlich gegen Beleuchtungsänderungen
\item Umrechnung von RGB nach HSI (falls $R=G=B$, dann ist $H$ undefiniert; falls $R=G=B=0$, dann ist $S$ undefiniert)
\end{itemize}
\begin{eqnarray*}
c &=& \textrm{arcos } \frac{2R - G - B}{2 \sqrt{(R-G)^2 + (R-B)(G-B)}} \\
H &=& \left\{ \begin{array}{cl} c & \textrm{ falls } B < G \\ 360\degree - c & \textrm{ sonst} \end{array} \right. \\
S &=& 1 - \frac{3}{R+G+B} \min (R,G,B) \\
I &=& \frac{1}{3} (R + G + B)
\end{eqnarray*}

\subsubsection{Hinterlegung}

\begin{itemize}
\item Hinterlegung eines 8bit Graustufen-Bildes im Speicher
\begin{itemize}
\item Pixel werden zeilenweise, von oben links nach unten rechts, linear abgelegt (Achtung: z.B. bei Bitmaps von unten links nach oben rechts)
\item Graustufen-Kodierung: ein Byte pro pixel; 0 schwarz, 255 weiß, dazwischen Graustufen
\end{itemize}
\item Hinterlegung eines RGB24 Farbbildes im Speicher
\begin{itemize}
\item Pixel werden zeilenweise, wie beim Graustufen-Bild, abgelegt
\item Farbkodierung: drei Bytes pro Pixel; für jeden Kanal gilt: 0 minimale, 255 maximale Intensität, dazwischen Nuancen
\end{itemize}
\end{itemize}

\subsubsection{Grauwert-Transformation}

Transformation von RGB24 nach 8bit Graustufen:
\begin{itemize}
\item Eine Möglichkeit: $g = (R+G+B)/3$, aber: menschliches Auge ist am empfindlichsten gegenüber der Farbe Grün.
\item Üblicherweise wird deshalb verwendet: $$g = 0,299 \cdot R + 0,587 \cdot G + 0,114 \cdot B$$
\end{itemize}

\subsubsection*{Bayer-Pattern\index{Bayer-Pattern}}

Sehr hochwertige Kameras, wie z.B. zum Filmen verwendet, besitzen drei Chips pro Pixel. Die meisten Farbkameras haben einen Chip pro pixel, der gegenüber der Farbe Rot, Grün oder Blau empfindlich ist. Bei "{}Ein-Chip-Kameras"{} wird meist das Bayer-Pattern verwendet:
\begin{itemize}
\item Um nach RGB24 zu konvertieren, muss interpoliert werden.
\item Die Empfindlichkeit einer Ein-Chip-Kamera ist um den Faktor 3 niedriger als die einer reinen Graustufen-Kamera.
\end{itemize}

\subsubsection*{Lochkameramodell\index{Lochkameramodell}}

\mypic{8}{lochkamera2}

Projektion eines Szenenpunktes $P = (X,Y,Z)$ auf einen Bildpunkt $p = (u,v,w)$ mit Brennweite $f$:
$$\frac{-u}{f} = \frac{X}{Z} \quad , \quad \frac{-v}{f} = \frac{Y}{Z} \quad , \quad w = -f \qquad \Rightarrow \qquad X = - \frac{uZ}{f} \quad , \quad - \frac{vZ}{f}$$
$$p = \left( \begin{array}{c} u \\ v \\ w \end{array} \right) = \left( \begin{array}{c} u \\ v \\ -f \end{array} \right) = - \frac{f}{Z} \left( \begin{array}{c} X \\ Y \\ Z \end{array} \right) = - \frac{f}{Z} P$$
Bei der Projektion geht die $Z$-Komponente verloren! \\
Beispiel mit 2 Kameras:
\mypic{8}{lochkamera1}

\subsubsection*{Lochkameramodell in Positivlage (Mattscheibenmodell)\index{Mattscheibenmodell}}

Einziger Unterschied Mattscheibenmodell $\Leftrightarrow$ Lochkameramodell:
\begin{itemize}
\item Projektionszentrum $C$ liegt hinter der Bildebene
\item dadurch: keine Spiegelung (Minuszeichen entfallen)
\end{itemize}

\subsubsection*{Linsensysteme\index{Linsensysteme}}

Moderne Kameras benutzen Linsen. \\ Vorteil:
\begin{itemize}
\item mehr Lichteinfall
\end{itemize}
Nachteile:
\begin{itemize}
\item nur Teile der Szene können gleichzeitig fokussiert werden
\item Linsenkrümmung führt zu Bildverzerrungen
\end{itemize}
Bild eines Objekts in Tiefe $Z$ wird in Abstand von der Linse $Z'$ gebildet mit: $$\frac{1}{Z} + \frac{1}{Z'} = \frac{1}{f} \qquad f \, : \, \textrm{Brennweite}$$
\begin{itemize}
\item Bei Einstellung der Bildebene auf $Z_0'$ wird ein gewisser Objektbereich um $Z_0$ "{}ausreichend scharf"{} abgebildet.
\item Beim Menschen: Änderung der Linsenbrennweite.
\item Bei Kameras: Verschieben der Linse gegenüber Bildebene
\item Vereinfachung wegen $Z >> Z'$: \\ $\Rightarrow$ Perpektivgleichungen aus dem Lochkameramodell können weiterverwendet werden! $$\frac{1}{Z} + \frac{1}{Z'} \thickapprox \frac{1}{Z'} \quad \Rightarrow \quad \frac{1}{Z'} \thickapprox \frac{1}{f}$$
\end{itemize}

\section{Bildverarbeitung}

\subsubsection*{Konzepte}

\begin{itemize}
\item Homogene Punktoperationen
\item Histogrammauswertung
\item Filterung
\item Geometrische Operatoren
\end{itemize}
$\to$ Unterdrückung von Störungen, "{}Verschönern"{} von Bildern, Verformen von Bildern

\subsubsection*{Homogene Punktoperatoren\index{Homogene Punktoperatoren}}

Anwendung: $$Img'(u,v) = f(Img(u,v))$$
Unabhängig von der Position bzw. den Nachbarn des Pixels. Implementierung der Funktion $f$ oftmals als Look-Up-Table (Hardware).

\subsubsection*{Affine Punktoperatoren\index{Affine Punktoperatoren}}

Affine Punktoperatoren:
\begin{eqnarray*}
f \, : \, [0 \dots q] &\to& [0 \dots q] \\ x &\mapsto& ax+b
\end{eqnarray*}
Parameter $a$ und $b$ legen die Funktion fest. Anwendungen:
\begin{itemize}
\item Kontrasterhöhung: $b=0$, $a > 0$ \\ Kontrastverminderung: $b=0$, $a < 0$
\item Helligkeitserhöhung: $b > 0$, $a = 1$ \\ Helligkeitsverminderung: $b < 0$, $a = 1$
\item Invertierung: $b = q$, $a = -1$
\item Kombinationen
\end{itemize}

Spreizung\index{Spreizung}:
\begin{itemize}
\item Intensitäten im Bild werden linear auf gesamten Bereich ausgedehnt
\item Abbildungsvorschrift: \[Img'(x,y) = q*\frac{Img(x,y)-min}{max-min}\]
\item Als affine Punktoperation: \[Img'(x,y) =  \frac{q}{max-min}*Img(x,y)-\frac{q*min}{max-min}\]
\end{itemize}

\subsubsection*{Nicht-Affine Punktoperationen\index{Nicht-Affine Punktoperationen}}

Beliebige Abbildungsfunktion $$f \, : \, [0 \dots q] \to [0 \dots q]$$
Anwendung:
\begin{itemize}
\item Ausgleich von Sensor-Nichtlinearitäten
\item Gewichtung
\item Binarisierung
\end{itemize}

\subsubsection*{Histogramme\index{Histogramme}}

Histogrammfunktion: gibt die Häufigkeit eines Selektionsmerkmals an. Normalerweise: Grauwert
$$H_{Img}(x) = \# (u,v) \, : \, Img(u,v) = x \, , \, x \in [0 \dots q]$$

Histogrammdehnung\index{Histogrammdehung}:
\begin{itemize}
\item Verbesserung der Spreizung
\item Anstatt $min$ und $max$ werden Quantile verwendet
\item Akkumuliertes Histogramm berechnen: \[H_a(x):=\sum\limits_{k=0}^x H(k)\]
\item \(H_q(p_{min})\) und \(H_q(p_{max})\) (z.B. \(p_{min} = 0.1\) und \(p_{max} = 0.9\)) bestimmen mit: \[H_q(p) := \inf\{ x \in \{ 0, \dots ,q\}:H_a(x) \geq p*H_a(q) \}\]
\item Falls \(H_q(p_{min}) = H_q(p_{max})\): Bild ist homogen
\item Sonst:
\[ a:=\frac{q}{H_q(p_{max}) - H_q(p_{min})} \]
\[ b:=-\frac{q*H_q(p_{min})}{H_q(p_{max}) - H_q(p_{min})} \]
\[ Img' \gets \text{AffinePunktoperation}(Img,a,b) \]
\end{itemize}

Histogrammausgleich\index{Histogrammausgleich}:
\begin{itemize}
\item Histogrammausgleich: bessere Anpassung an das Sehvermögen des Menschen; kein Informationsgewinn \\
$H_n(0)$ \verb|:=| $H_{Img}(0)$ \\
\verb|for| $x$ \verb|:=| $1$ \verb|to| $q$ \verb|do| \\
\verb|  |$H_n(x)$ \verb|:=| $H_n(x-1) + H_{Img}(x)$ \\
\verb|endfor|
\item Automatische Kontrast- und Helligkeitsapassung durch Histogrammanalyse \\
$H_n(0)$ \verb|:=| $H_{Img}(0)$ \\
\verb|for| $x$ \verb|:=| $1$ \verb|to| $q$ \verb|do| \\
\verb|  |$H_n(x)$ \verb|:=| $H_n(x) \cdot \frac{q}{width \cdot heigth}$ \\
\verb|endfor|
\item Bei bereits berechnetem Histogramm $H_{Img}(x)$ kann der Histogrammausgleich durch den folgenden Algorithmus berechnet werden: \\
$H_n(0)$ \verb|:=| $H_{Img}(0)$ \\
\verb|for| $y$ \verb|:=| $0$ \verb|to| $height - 1$ \verb|do| \\
\verb|  for| $x$ \verb|:=| $0$ \verb|to| $width-1$ \verb|do| \\
\verb|    | $Img'(u,v)$ \verb|:=| $H_n(Img(u,v))$ \\
\verb|  endfor| \\
\verb|endfor|
\end{itemize}

\subsubsection*{Heute}

\begin{itemize}
\item Bildanalyse durch Frequenzanalyse
\item Filter in Frequenzbereich
\item Filter in Ortsbereich
\end{itemize}

\subsection{Bildanalyse durch Frequenzanalyse}

\begin{itemize}
\item (Grauwert-) Bilder lassen sich signaltheoretisch als Summe verschiedenfrequenter Signale betrachten.
\item Niedrige Frequenzen: Schwache Grauwertübergänge
\item Hohe Frequenzen: Scharfe Grauwertübergänge
\item Nützlich z.B. zum Finden gerader Linien
\end{itemize}
$\to$ Fourier-Analyse!

\subsubsection*{2-Dim Fouriertransformation}

Kontinuierliches 2-dimensionales Signal: $$F(u,v) = \int\limits_{x = - \infty}^{\infty} \int\limits_{y = - \infty}^{\infty} f(x,y) e^{-2i\pi (ux + vy)} dxdy$$
Diskretes 2-dimensionales Signal: $$F(u,v) = \sum\limits_{x = - \infty}^{\infty} \sum\limits_{y = - \infty}^{\infty} f[x,y] e^{-2i\pi (ux + vy)T}$$

\subsubsection*{Fouriertransformation in der Bildverarbeitung}

DFT bei Bild mit ImgSize $[0 \leq x \leq M][0 \leq y \leq N]$: $$F(u,v) = \sum\limits_{y=0}^{N-1} \left( \sum\limits_{x=0}^{M-1} f[x,y] e^{-2i\pi \frac{ux}{M}} \right) \cdot e^{-2i\pi \frac{vy}{N}}$$
DFT bei quadratischem Bild mit ImgSize $[0 \leq x \leq N][0 \leq x \leq N]$: $$\sum\limits_{y=0}^{N-1} \sum\limits_{x=0}^{N-1} f[x,y] e^{\frac{-2i\pi (ux + vy)}{N}}$$
Anschaulich: Durchführung der 1D-DFT auf jeder Zeile und Speicherung der Daten in Matrix (inndere Klammer). Durchführung der 1D-DFT auf jeder Spalte der ermittelten Matrix.

\begin{itemize}
\item Gewichtete Summation aller Bildpunkte
\item Zerlegung des Bildes in Sinus- und Cosinusfunktionen
\item Je weiter ein Punkt im Spektrum vom Bildmittelpunkt entfernt ist, desto höher ist seine darstellende Frequenz $u$ bzw. $v$.
\item D.h. im Bildinneren tiefe Frequenzen, in äußeren Bereichen hohe Frequenzen
\item Anwendung:
\begin{itemize}
\item Bildanalyse (z.B. Muster-, Geschwindigkeitserkennung)
\item Bildfilterung (z.B. Tiefpass)
\item Bildkompression (z.B. in jpeg Format)
\end{itemize}
\end{itemize}

\subsubsection*{Fouriertransformation}

\begin{itemize}
\item Die Variablen $u$ und $v$ heissen Frequenzvariablen
\item $F(u,v)$ ist komplexe Funktion
\item $F(u,v)$ ist darstellbar als 2 Bilder
\begin{itemize}
\item in Realteil und Imaginärteil $$F(u,v) = R(u,v) + I(u,v)$$
\item oder Betrag (auch Spektrum genannt, oft logarithmisch dargestellt) und Phase $$F(u,v) = |F(u,v)| \cdot e^{i \varphi (u,v)}$$
\end{itemize}
\item Quadrat des Spektrums heisst spektrale Dichte.
\end{itemize}

\subsection{Bildbearbeitung}

\begin{itemize}
\item Durch Filter im Ortsbereich oder Transferfunktionen im Frequenzbereich.
\item Ortsbereich:
\begin{itemize}
\item Manipulation von Grauwerten
\item anschaulich
\item häufig: Punktoperationen, Glättung
\end{itemize}
\item Frequenzbereich:
\begin{itemize}
\item Manipulation der Frequenzanteile
\item keine unmittelbare bildliche Vorstellung
\item häufig: starke Glättung, frequenzselektive Filter
\end{itemize}
\end{itemize}

\subsubsection*{Bildbearbeitung im Ortsbereich}

Faltung zweier Funktionen 1D kontinuierlich: $$h(x) = f(x) * g(x) = \int\limits_{a = - \infty}^{\infty} f(a) g(x-a) da$$
Faltung zweier Funktionen 1D zeitdiskret: $$h[x] = f[x] * g[x] = \sum\limits_{a = - \infty}^{\infty} f[a] g[x-a]$$
Faltung zweier Funktionen 2D kontinuierlich: $$h(x,y) = f(x,y) * g(x,y) = \int\limits_{a = -\infty}^{\infty} \int\limits_{b = -\infty}^{\infty} f(a,b) g(x-a,y-b) dadb$$
Faltung zweier Funktionen 2D zeitdiskret: $$h[x,y] = f[x,y] * g[x,y] = \sum\limits_{a = -\infty}^{\infty} \sum\limits_{b = -\infty}^{\infty} f[a, g[x-a,y-b]$$

\subsubsection*{Faltung}

\begin{itemize}
\item Bildmatrizen werden in den relevanten Randbereichen mit Nullen gefüllt.
\item Der neue Bildwert ist eine gewichtete Summe der Pixel die unter der gespiegelten Matrix liegen.
\item Als Gewichte dienen die Matrizenwerte.
\item Bildfilterung ist die Faltung einen Bildes mit einer Filtermatrix bzw. Maske.
\item Die Transferfunktion $H(u,v)$ ist die Fouriertransformierte der Filterfunktion $h(x,y)$.
\end{itemize}

\subsubsection*{Filteroperationen}

\begin{itemize}
\item Glättungsoperatoren (Rauschelimination): Mittelwertfilter, Gauß
\item Kantendetektoren: Prewitt, Sobel, Roberts, Laplace
\item kombinierte Filter: Laplacian of Gauß
\end{itemize}

\subsection{Filter}

\subsubsection*{Gauß\index{Gauß-Filter}}

Gaußfilter mit Radius $\sigma$ definiert durch die Gaußfunktion $$G(x,y) = \frac{1}{2 \pi \sigma^2} e^{-\frac{x^2 + y^2}{2\sigma^2}}$$

\subsubsection*{Mittelwrtfilter\index{Mittelwertfilter}}

Ziel: Störunterdrückung \\ Beispiel: Durchschnitt aus 8-Umgebung und Punkt. Größe beliebig wählbar.
$$m'(x,y) = \sum\limits_{j=-1}^1 \sum\limits_{i=-1}^1 m(i,j) \cdot p(x-j,y-i)$$

\subsubsection*{Anwendung der nachfolgenden Filter}
am Beispiel des Sobel-X Filters:
\mypic{12}{filterbsp}

\subsubsection*{Prewitt\index{Prewitt-Filter}}

\textbf{Prewitt-X Filter} $$P_x = \frac{\partial g(x,y)}{\partial x}$$ approximiert durch $$p_x = \left( \begin{array}{ccc} -1 & 0 & 1 \\ -1 & 0 & 1 \\ -1 & 0 & 1 \end{array} \right)$$
Kantendetektion: vertikal gut, horizontal schlecht \\
\textbf{Prewitt-Y Filter} $$P_y = \frac{\partial g(x,y)}{\partial y}$$ approximiert durch $$p_y = \left( \begin{array}{rrr} -1 & -1 & -1 \\ 0 & 0 & 0 \\ 1 & 1 & 1 \end{array} \right)$$
Kantendetektion: vertikal schlecht, horizontal gut \\
\textbf{Prewitt-Operator}
\begin{itemize}
\item Kombination der Prewitt-Filter zur Bestimmung des Grauwertgradientenbetrages $M$: $$M \thickapprox \sqrt{P_x^2 + P_y^2}$$
\item Danach: Schwellwertfilterung
\end{itemize}

\subsubsection*{Sobel\index{Sobel-Filter}}

\textbf{Sobel-X Filter} $$S_x = \frac{\partial g(x,y)}{\partial x}$$ approximiert durch $$s_x = \left( \begin{array}{ccc} -1 & 0 & 1 \\ -2 & 0 & 2 \\ -1 & 0 & 1 \end{array} \right)$$
Kantendetektion: vertikal gut, horizontal schlecht \\
\textbf{Sobel-Y Filter} $$S_y = \frac{\partial g(x,y)}{\partial y}$$ approximiert durch $$s_y = \left( \begin{array}{rrr} -1 & -2 & -1 \\ 0 & 0 & 0 \\ 1 & 2 & 1 \end{array} \right)$$
Kantendetektion: vertikal schlecht, horizontal gut \\
\textbf{Sobel-Operator}
\begin{itemize}
\item Kombination der Sobel-Filter zur Bestimmung des Grauwertgradienten-Betrages $M$: $$M \thickapprox \sqrt{S_x^2 + S_y^2}$$
\item Danach: Schwellwertfilterung
\end{itemize}

\subsubsection*{Roberts\index{Roberts-Filter}}

$$R(g(x,y)) = |R_x(g(x,y))| + |R_y(g(x,y))|$$ wobei $$R_x = \left( \begin{array}{rr} -1 & 0 \\ 0 & 1 \end{array} \right) \quad , \quad R_y = \left( \begin{array}{rr} 0 & -1 \\ 1 & 0 \end{array} \right)$$
Kantendetektion: diagonal gut

\subsubsection*{Laplace\index{Laplace-Filter}}

Laplace-Operator: $$\nabla^2 g(x,y) = \frac{\partial^2 g(x,y)}{\partial x^2} + \frac{\partial^2 g(x,y)}{\partial y^2}$$ wobei $$\nabla^2 \thickapprox \left( \begin{array}{rrr} 0 & 1 & 0 \\ 1 & -4 & 1 \\ 0 & 1 & 0 \end{array} \right)$$
Kantendetektion: Nulldurchgänge markieren Kanten, Subpixelgenauigkeit erreichbar \\ Näherung des Laplace-Operators: $$\nabla^2 \thickapprox \left( \begin{array}{rrr} 1 & 4 & 1 \\ 4 & -20 & 4 \\ 1 & 4 & 1 \end{array} \right)$$
Kantendetektion: Stärkere Kanten, aber mehr Störkanten

\subsubsection*{Laplacian of Gauß (LoG)\index{Laplacian of Gauß-Filter}}

Der Laplace-Operator ist gegen Rauschen sehr empfindlich. Wesentlich bessere Ergebnisse erhält man, wenn man das Bild zunächst mit einem Gauß-Filter glättet und danach den Laplace-Operator anwendet. $$LoG(g(x,y)) = \nabla^2 (G(x,y) * g(x,y))$$ Approximation (Faltung mit Matrix): $$\nabla^2 G(x,y) = \left( \begin{array}{rrrrr} 0 & 0 & -1 & 0 & 0 \\ 0 & -1 & -2 & -1 & 0 \\ -1 & -2 & 16 & -2 & -1 \\ 0 & -1 & -2 & -1 & 0 \\ 0 & 0 & -1 & 0 & 0 \end{array} \right)$$
Kantendetektion: Stärkere Kanten,weniger Rauschen

\subsubsection*{Canny-Kantendetektor\index{Canny-Kantendetektor}}
\begin{enumerate}
\item Rauschunterdrückung: Gauß-Filter
\item Kanten detektieren:
	\begin{itemize}
	\item Prewitt oder Sobel
	\item Zusammenrechnen mit \(B' = \sqrt{B_x^2 + B_y^2}\)
	\item Gradientenrichtung bestimmen: \(\Phi(x,y) = arctan\left(\frac{B_y(x,y)}{B_x(x,y)}\right)\)
	\item Runden auf 0°, 45°, 90° oder 135°
	\end{itemize}
\item Non-Maximum Surpression:
	\begin{itemize}
	\item Für jeden Pixel in Gradientenrichtung schauen, ob das Pixel davor oder dahinter einen höheren Wert hat. Falls ja, dann Pixel auf 0 setzen, falls nein, dann beibehalten.
	\end{itemize}
\item Hysterese-Schwellwerverfahren:
	\begin{itemize}
	\item Verwende zwei Schwellwerte \(T_1\) und \(T_2\) mit \(T_1 \leq T_2\)
	\item Markiere alle Pixel mit Werten größer \(T_2\) als Kantenpixel
	\item Setze alle Pixel mit Werten kleiner \(T_1\) auf 0
	\item Beginnend bei jedem Kantenpixel:
		\begin{itemize}
		\item Verfolge alle angrenzenden Kanten, solange Wert \(\geq T_1\)
		\item Markiere alle dazugehörigen Pixel als Kanten
		\end{itemize}
	\item Setze alle noch nicht als Kante markierten Pixel auf 0
	\end{itemize}
\end{enumerate}


\subsection{2D Bildverarbeitung}

\subsubsection*{Sensorische Erfassung}

Aufgaben der sensorischen Umwelterfassung:
\begin{itemize}
\item Wiedererkennung bekannter Sachverhalte: Objekte, Personen, Orte
\item Erlernen neuer Sachverhalte
\item Erkennung der eigenen Bewegung
\end{itemize}
Verfahren zur Lösung dieser Aufgaben:
\begin{itemize}
\item Sensorische Primitive, Segmentierung
\item Annahmen, Einschränkungen
\item Lernverfahren
\end{itemize}

\subsubsection*{Segmentierung\index{Segmentierung}}

Segmentierung ist die Aufteilung eines Bildes in aussagekräftige Bestandteile. Erlaubt:
\begin{itemize}
\item Aussagen über das Bild
\item Reduktion der Datenmenge
\item Verfolgung von Merkmalen über die Zeit / mehrere Sensoren
\end{itemize}
Beliebt sind: Kanten, Ecken, Textur, Farbe

\subsubsection*{Schwellwertfilterung\index{Schwellwertfilterung}}

Schwellwertfilterung zur Konvertierung eines Grauwertbildes in ein binäres Bild. Ziel: Trennung interessanter Objekte vom Hintergrund. $$Img'(u,v) = \left\{ \begin{array}{cl} q & \textrm{ falls } Img(u,v) \geq T \\ 0 & \textrm{ sonst} \end{array} \right.$$

\subsubsection*{Farbe}

Oft können Objekte über ihre Farbe segmentiert werden:
\begin{itemize}
\item menschliche Hautfarbe
\item einheitlich gefärbte Objekte
\end{itemize}
Problem:
\begin{itemize}
\item wechselnde Lichtbedingungen
\item Reflexionen, Schattenwürfe
\end{itemize}
Verfahren:
\begin{itemize}
\item Histogrammbasiert (z.B. in RGB, HSV bzw. RG, HS) \\ HS-Farbhistogramm:
\begin{itemize}
\item Weglassen des $I$-Kanals ergibt 2D-Histogramm
\item Training eines Klassifikators auf dem Histogramm
\end{itemize}
\item mit Hilfe der Mahalanobis-Distanz (z.B. in RGB): \\ gegeben: $x_i = (R,G,B)^T$ sind manuell positiv klassifizierte Pixel
\begin{eqnarray*}
C &=& \frac{1}{n-1} \sum\limits_{i=1}^n (x_i - \overline{x})(x_i - \overline{x})^T \qquad \textrm{Kovarianzmatrix} \\ p(x) &=& e^{- \frac{1}{2 \sigma^2} x^T C^{-1} x} \qquad \textrm{Berechnung der Farbwahrsch.}
\end{eqnarray*}
\item Klassifikation durch Verwendung von Neuronalen Netzen
\item durch Intervallschranken im HSI-Farbraum:
$$f(H,S,V) = H \geq H_{\min} \wedge H \leq H_{\max} \wedge S \geq S_{\min} \wedge S \leq S_{\max} \wedge V \geq V_{\min} \wedge V \leq V_{\max}$$
\end{itemize}

\subsubsection*{Morphologische Operatoren\index{Morphologische Operatoren}}

\begin{itemize}
\item Wähle Struckturelement, z.B. Quadrat mit 3x3 Pixel
\item Wandere mit Struckturelement über Bild und führe Operation durch
\item \textbf{Dilatation}\index{Dilatation} ist ein Operation auf Binärbildern. Alle Bereiche, die farbig (1 und nicht 0, da Binär) sind, werden ausgedehnt. Immer wenn unter dem mittleren Pixel des Struckturelements ein farbiges Pixel ist, werden alle Pixel unter dem Struckturelement als farbig markiert.
\item \textbf{Erosion}\index{Erosion} ist die Komplemetäroperation zu Dilatation. Nur falls alle Pixel unter dem Struckturelement farbig sind, wird das Element in der Mitte farbig gelassen und alle anderen auf 0 gesetzt. Falls nicht alle Pixel farbig sind, werden alle auf 0 gesetzt.
\item Öffnen-Operation\index{Öffnen-Operation}: Zuerst Erosion und anschließend Dilatation anwenden.
\item Schließen-Operation\index{Schließen-Operation}: Zuerst Dilatation und anschließend Erosion anwenden.

\end{itemize}


\subsubsection*{Bewegung}

Einfacher Ansatz: Differenzbilder
\begin{itemize}
\item Subtraktion aufeinander folgender Bilder einer Video-Sequenz: $$Img_t'(u,v) = |Img_t(u,v) - Img_{t-1}(u,v)|$$
\item Anschließend kann auf $Img_t'$ Schwellwertfilterung durchgeführt werden
\item Regionen, in denen sich etwas bewegt, erscheinen weiß; ruhige Regionen erscheinen schwarz
\item Bewegung in homogenen Regionen wird nicht erkannt (Kanten, Textur sind notwendig)
\item Richtung der Bewegung wird nicht erkannt
\end{itemize}
Differenzbilder werden auch für Hintergrundsubtraktion verwendet, dann wird $Img_{t-1}$ durch ein festes $Img_0$ ersetzt. Weitere Ansätze: Optical Flow und Erweiterungen

\subsubsection*{Region Growing\index{Region Growing}}

gegeben: Graustufen-Bild, gesucht: zusammenhängende Regionen \\ Algorithmus in Pseudocode:
\begin{enumerate}
\item wähle Saatpunkt $p_0 = (u_0,v_0)$
\item initialisiere Region $R = \{ p_0 \}$, wähle Schwelle $\varepsilon$
\item solange $\exists p \in R$, $q \not\in R$ mit $||p-q|| \leq 1$ und $|Img(p_0) - Img(q)| \leq \varepsilon$ mache $R = R \cup \{ q \}$
\end{enumerate}

\subsubsection*{Kanten}

Vom Menschen konstruierte Umgebungen:
\begin{itemize}
\item gut strukturiert
\item viele gerade Linien (Wände, Türen, Schränke)
\item einfache Segmentierung / 3D-Rekonstruktion
\item viele Informationen in einem einzigen Merkmal
\end{itemize}
Vorgehensweise: $$\textrm{Bildaufnahme} \quad \Rightarrow \quad \textrm{Filtern, Binarisieren} \quad \Rightarrow \quad \textrm{Pixel } \to \textrm{ Kantensegmente}$$

\subsubsection*{Pixel $\to$ Kanten}

\textbf{Iterative Endpoint Fit:}\index{Iterative Endpoint Fit} \\[0,1cm]
gegeben: Punkte $P$, Linien $L = \{ \}$, Distanzschwelle $d$
\begin{itemize}
\item Fine $x_1$, $x_2$ aus $P$ mit $||x_1 - x_2|| = \max$; \\ verbinde sie durch Linie $l_0 = \{x_1,x_2\}$; $L=L \cup \{l_0\}$
\item Entferne $x_1$, $x_2$ aus $P$
\item Für alle $l \in L$:
\begin{itemize}
\item Finde $x \in P$ mit $||l-x|| = \max$
\item Wenn $||l-x|| < d$:
\begin{itemize}
\item Ordne $x$ als Mitgliedpunkt $l$ zu
\item Entferne $x$ aus $P$
\end{itemize}
\item Sonst
\begin{itemize}
\item Brich $l$ in $l_1 = \{ x_1 , x\}$ und $l_2 = \{x , x_2 \}$ auf
\item Andere Mitgliedspunkte von $l$ wieder in $P$
\end{itemize}
\item $P$ leer $\Rightarrow$ Abbruch, sonst weiter
\end{itemize}
\item Lösche Linien mit weniger als $n$ Punkten
\end{itemize}

\textbf{Hough-Transformation:}\index{Hough-Transformation}
\begin{itemize}
\item Ziel: Erkennung gerader Linien im Bild
\item Ansatz: Stelle Linie durch Normalenvektor (Länge, Winkel) in Polarkoordinaten dar (Sinus-Kosinus-Kurve)
\item Kurven für kollineare Punkte schneiden sich in genau zwei Punkten $$r = x \cdot \cos(\theta) + y \cdot \sin(\theta)$$
\item Transformation in den Hough-Raum: Additives Eintragen aller Sinus-Kosinus-Kurven für alle Pixel in ein Histogramm
\item Finden der Maxima bzw. Cluster von "{}Treffern"{} im Hough-Raum
\item Brute-Force-Ansatz; für ein $n \times n$-Bild liegt die Laufzeit in $O(n^3)$
\end{itemize}
Unterschied zur Regressionsanalyse: Die Bestimmung der Regressionsgerade ist das geeignetere Verfahren, wenn schon klar ist, welche Pixel eine Gerade bilden sollen; dann ist die Regressionsgerade die optimale Gerade im Sinne der Summe der Fehlerquadrate. Die Regressionsgerade kann jedoch keine Segmentierung vornehmen. Dahingegen lassen sich mit der Hough-Transformation in beliebigen Bildern geradlinige Strukturen berechnen; die punkte dazu müssen jedoch verhältnismäßig exakt auf einer Linie liegen.

\subsubsection*{Punktmerkmale}

Kanten/Konturen/Farbe können nicht immer für die Segmentierung herangezogen werden. Texturierte Objekte lassen sich in der Regel nicht durch die bislang vorgestellten Verfahren segmentieren. Lösung: Verwendung von \textsl{lokalen} Punktmerkmalen (auch genannt: Textmerkmale):
\begin{itemize}
\item Harris Corner Detector
\item Shi-Tomasi Features
\item SIFT-Features
\item Maximally Stable Extremal Regions
\end{itemize}
Punktmerkmal: $(2n+1) \times (2n+1)$-Pixel-Block um Pixel $p$. Fast immer basierend auf Grauwertbildern. Gewünschte Eigenschaft: Wiedererkennbarkeit \\ $\Rightarrow$ hoher Gradient in mehrerer Richtungen \\[0,1cm]
\textbf{Harris Corner Detector}\index{Harris Corner Detector}: \\
Sind die Eigenwerte der Matrix $$A = \left( \begin{array}{cc} \left( \frac{\partial Img(x,y)}{\partial x} \right)^2 & \frac{\partial Img(x,y)}{\partial x} \frac{\partial Img(x,y)}{\partial y} \\ \frac{\partial Img(x,y)}{\partial x} \frac{\partial Img(x,y)}{\partial y} & \left( \frac{\partial Img(x,y)}{\partial y} \right)^2 \end{array} \right)$$ groß, dann verursacht eine kleine Bewegung in beliebiger Richtung eine große Grauwertänderung. \\ Finden von Ecken durch Suche nach lokalen Maxima in: $$R = det(A) - k \cdot trace(A)^2 \quad , \quad k \thickapprox 0,04$$
Häufiges Problem:
\begin{itemize}
\item Wiederfinden bzw. Zuordnung von Punktmerkmalen für:
\begin{itemize}
\item Objekterkennung auf der Basis von Punktmerkmalen
\item Stereo-Sehen bzw. "{}Structure from Motion"{} (Korrespondenzproblem)
\end{itemize}
\item Lösung des Korrespondenzproblems erfolgt für Punktmerkmale durch Korrelationsverfahren:
\begin{itemize}
\item \textbf{Sum of Squared Differences}\index{Sum of Squared Differences} (SSD) wird minimal bei guter Übereinstimmung: $$\sum\limits_{i = -n}^n \sum\limits_{j = -n}^n (Img_0(x-i,y-j) - Img_1(x-i,y-j))^2$$
\item \textbf{Sum of Absolute Differences}\index{Sum of Absolute Differences} (SAD) wird minimal bei guter Übereinstimmung: $$\sum\limits_{i = -n}^n \sum\limits_{j = -n}^n |Img_0(x-i,y-j) - Img_1(x-i,y-j)|$$
\item \textbf{(Zero Mean) Cross Correlation}\index{(Zero Mean) Cross Correlation} wird maximal bei guter Übereinstimmung: $$\sum\limits_{i = -n}^n \sum\limits_{j = -n}^n (Img_0(x-i,y-j)- \overline{Img_0})(Img_1(x-i,y-j)- \overline{Img_1})$$ wobei $$\overline{Img} \, : \, \textrm{Durchschnitt}$$
\item \textbf{Zero Mean Normalized Cross Correlation}\index{Zero Mean Normalized Cross Correlation} wird maximal bei guter Übereinstimmung: $$\frac{\sum\limits_{i=-n}^{n}\sum\limits_{j=-n}^{n}Img_1(u_1 + i, v_1 + j) - \overline{Img_1}(u_1,v_1,n)) \cdot (Img_2(u_2 + i, v_2 + j) - \overline{Img_2}(u_2,v_2,n))}{\sqrt{\sum\limits_{i=-n}^n \sum\limits_{j=-n}^n (Img_1(u_1 + i, v_1 + j) - \overline{Img_1}(u_1,v_1,n))^2 \sum\limits_{i=-n}^n \sum\limits_{j=-n}^n Img_2(u_1 + i, v_1 + j) - \overline{Img_2}(u_1,v_1,n))^2} }$$
wobei $$\overline{Img}(u,v,n) = \frac{1}{(2n+1)^2} \sum\limits_{i=-n}^n \sum\limits_{j=-n}^n Img(u+i,v+j)$$
\end{itemize}
\end{itemize}
Für die Objekterkennung geschieht das Wiederfinden oftmals unter Verwendung:
\begin{itemize}
\item der Hauptkomponentenanalyse oder engl. \textsl{Principal Component Analysis} (PCA) und Nearest-Neighbor- bzw. $k$-Nearest-Neighbor-Klassifikator
\item von Neuronalen Netzen
\item oder Kombinationen (zuerst PCA zur Kompression, dann SVM (\textsl{Support Vector Machine}) zur Klassifikation)
\end{itemize}

\subsection{Geometrische 2D-Transformationen}

\subsubsection*{Translation}\index{Translation}

Translation eines 2D-Vektors: $$\left( \begin{array}{c} x_0 \\ y_0 \end{array} \right) + \left( \begin{array}{c} x \\ y \end{array} \right) = \left( \begin{array}{c} x_0 + x \\ y_0 + y \end{array} \right)$$

\subsubsection*{Rotation}\index{Rotation}

\begin{itemize}
\item o.B.d.A. auf Einheitsvektor zurückführbar (Basistransformation)
\item Konvention: Rechtskoordinatensystem
\item Rotation von $(x_0,y_0)$ um Winkel $\beta$ mit Ergebnis $(x,y)$:
\end{itemize}
Aus Additionstheorem:
\begin{eqnarray*}
x &=& \cos(\alpha + \beta) = \cos(\alpha) \cos(\beta) - \sin(\alpha) \sin(\beta) \\ y &=& \sin(\alpha + \beta) = \sin(\beta) \cos(\alpha) + \cos(\beta) \sin(\alpha)
\end{eqnarray*}
und mit $(x_0,y_0) = (\cos(\alpha), \sin(\alpha))$:
$$\left( \begin{array}{c} x \\ y \end{array} \right) = \left( \begin{array}{rr} \cos(\beta) & -\sin(\beta) \\ \sin(\beta) & \cos(\beta) \end{array} \right) \left( \begin{array}{c} x_0 \\ y_0 \end{array} \right)$$

\subsubsection*{Homogene Koordinaten}

Homogene Koordinaten $$h = (h_0,h_1, \dots, h_i, h_{i+1})$$ eines Punktes $p$ im $R^i$ mit $$p = (p_0,\dots,p_i)$$ sind Zahlen, für die gilt: $$p_k = \frac{h_k}{h_{i+1}} \quad \forall 0 \leq k \leq i$$

\subsubsection*{Homogene 2D-Transformationen}

Transformationen definiert durch Rotation $R$ und Translation $t$.
$$\myvectwo{x}{y} = R \myvectwo{x_0}{y_0} + t = \myvecfour{r_{11}}{r_{12}}{r_{21}}{r_{22}} \myvectwo{x_0}{y_0} + \myvectwo{t_x}{t_y}$$
Darstellung mit Hilfe homogener Koordinaten und einer geschlossenen Transformationsmatrix:
$$\myvecthree{x}{y}{1} = \left( \begin{array}{cc|c} & R & t \\ \hline 0 & 0 & 1 \end{array} \right) \myvecthree{x_0}{y_0}{1} = \left( \begin{array}{cc|c} r_{11} & r_{12} & t_x \\ r_{21} & r_{22} & t_y \\ \hline 0 & 0 & 1 \end{array} \right) = A \myvecthree{x_0}{y_0}{1}$$

\subsubsection*{Partikel Filter und 2D-Tracking}

Für Tracking-Applikationen wird oftmals das Kalmanfilter, das Partikel Filter oder Kombinationen aus beiden verwendet. Vorteile des Partikel Filters gegenüber dem Kalmanfilter:
\begin{itemize}
\item Es kann automatisch mehrere, parallel existierende Hypothesen halten $\Rightarrow$ geringere Gefahr in lokalen Minima zu verharren.
\item Es kann beliebige Wahrscheinlichkeitsdichten modellieren und dadurch nichtlineare Bewegungen auf natürliche Weise verfolgen.
\end{itemize}
Nachteil: Rechenaufwand ist proportional zur Anzahl der notwendigen Partikel, welche (bei konstanter Auflösung) mit der Dimension des Konfigurationsraums exponentiell steigt. \\ Im Kern des Partikel Filters befinden sich ein Modell mit Konfigurationsraum $R^n$. \\ \textit{Eingabe:} Beobachtungen $z$; hier: Bilder einer Videosequenz \\ \textit{Ausgabe:} Schätzung der Konfiguration $s \in R^n$, die den aktuellen Beobachtungen $z$ entspricht \\ \textit{Zentrale Funktion:} Bewertungsfunktion $\myprob{z}{s}$, die die a-posteriori Wahrscheinlichkeit berechnet, dass $s$ die zu $z$ passende Konfiguration ist. \\
Das Partikel Filter modelliert die Wahrscheinlichkeitsdichtefunktion oder engl. \textsl{probability density function (pdf)} durch eine feste Anzahl von $N$ Partikeln. Die Wahrscheinlichkeitsdichtefunktion beschreibt die a-posteriori Wahrscheinlichkeiten für den Konfigurationsraum. Jedes Partikel ist ein Paar $(s_i, \pi_i)$, wobei $s_i$ eine Konfiguration ist und $\pi_i$ die dazugehörige a-posteriori Wahrscheinlichkeit mit $\sum_{i=1}^n \pi_i = 1$. Die aktuelle Schätzung des Partikel Filters erfolgt über das gewichtete Mittel über alle Partikel: $$\overline{s} = \sum\limits_{i=1}^{N} \pi_i \cdot s_i$$

\textbf{\textsl{Algorithmus}}

\begin{enumerate}
\item Initialisiere alle $N$ Partikel (z.B. mit Gleichverteilung)
\item Verarbeite neue Beobachtungen (z.B. Vorverarbeitung neuer Bilder)
\item Ziehe $N$ Partikel aus der letzten Generation, proportional zu ihrer Wahrscheinlichkeit $\pi_i$, und für jedes dieser Partikel:
\begin{itemize}
\item Berechne neue Konfiguration auf Basis der alten Konfiguration durch Addition normalverteilten Rauschens und evtl. durch Hinzunahme eines dynamischen Modells.
\item Berechne für diese neue Konfiguration die neue a-posteriori Wahrscheinlichkeit mit Hilfe der Bewertungsfunktion $\myprob{z}{s}$
\end{itemize}
\item Berechne aktuelle Schätzung $\overline{s}$ über alle Partikel
\item Fahre fort mit Schritt 2
\end{enumerate}

\textbf{\textsl{Einfaches Beispiel für eine Bewertungsfunktion}}

\begin{itemize}
\item Anwendung: \\ 2D-Tracking einer dichten, in etwa quadratischen Fläche von in etwa fester Größe in einem binarisierten Bild
\item Modell: \\ Quadrat fester Größe mit Kantenlänge $k$ mit Konfigurationsraum $R^2$ (Koordinaten $u,v$ im Bild)
\item Bewertungsfunktion: $$\myprob{z}{s} \propto e^{- \frac{1}{2 \sigma^2} \left( k^2 - \sum_{m \in M} g_m \right)}$$ wobei $M$ die Menge aller Pixel im binarisierten Bild $z$ (Beobachtungen) im durch $s$ (Konfiguration) definierten Quadrat beschreibt und $g_m$ deren Intensität aus $\{0,1\}$.
\end{itemize}

\subsection{Fragen zum Kapitel}
\begin{enumerate}
  %SS 12 Klausur
	\item Was ist der maximale Korrelationswert, den die Zero Mean Normalized Cross Correlation (ZNCC) liefern kann?
	%WS 12 Klausur
	\item Warum kann man mit Hilfe der Epipolargeometrie das Sterokorrespondenzproblem schneller lösen?
\end{enumerate}

% Fähigkeiten
% 1. Anwendung aller besprochener Filter
% 2. Quaternionen rechnen
% 3. Rechnen mit Rotationen und Translationen
% 4. Eine Spreizung vornehmen können







