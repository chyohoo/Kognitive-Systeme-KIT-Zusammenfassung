% !TeX root = summary.tex

\newcommand{\myfbox}[1]{
\begin{tabular}{|l|}
\hline #1 \\ \hline
\end{tabular}
}

\chapter{Bildverarbeitung}

\section{Bildgenerierung}

\begin{itemize}
\item Digitalkameras (meist CCD, aber auch CMOS)
\item Echt (Raum/Zeit kont) -(CCD-Kamera)-> Analog PAL/NTSC (Raum disk, Zeit kont) -(Frame-Grabber)-> Bild (Raum/Zeit disk)
\begin{itemize}
\item Format unterschiedlich, (S/W 8bit Graustufen, Farb -> bayer-Pattern, RGB24, YUV422
\end{itemize}
\item Kameras unterscheiden sich durch:
\begin{itemize}
\item Bildqualität, Farben, Auflösung, Farbkodierung (bits), max Framerate
\end{itemize}
\end{itemize}

\section{Bildrepräsentation}

\textbf{Monochrombild:} Diskrete Funktion
\begin{eqnarray*}
Img \, : \, [0 \dots n-1] \times [0 \dots m-1] &\to& [0 \dots q] \\ (u,v) &\mapsto& Img(u,v)
\end{eqnarray*}
Üblich: $q = 255$; $n = 640$, $m=480$ (VGA) oder $n = 768$, $m=576$ (PAL) \\[0,1cm]
\begin{itemize}
\item zeilenweise linear, Grau -> 1 Byte/Pixel, RGB -> 3 Byte/Pixel
\end{itemize}

\subsection{Farbbild}
\begin{itemize}
\item versch. Farbmodelle, Klassifikation nach erreichbarem Farbraum
\begin{itemize}
\item S/W, RGB (->Monitore), HSI (-> Farbsegmentierung), CIE (Wellenlänge), CMYK
\end{itemize}
\end{itemize}

\subsubsection{HSI-/HSV-Modell\index{HSI-/HSV-Modell}}

\begin{itemize}
\item Hue (Farbnuancen), Saturation (Sättigung), Intensity/Value (Helligkeit)
\item trennt Helligkeit vom Farbwert $\Rightarrow$ unempfindlich gegen Beleuchtungsänderungen
\item Umrechnung von RGB nach HSI (falls $R=G=B$, dann ist $H$ undefiniert; falls $R=G=B=0$, dann ist $S$ undefiniert)
\end{itemize}
\begin{eqnarray*}
c &=& \textrm{arcos } \frac{2R - G - B}{2 \sqrt{(R-G)^2 + (R-B)(G-B)}} \\
H &=& \left\{ \begin{array}{cl} c & \textrm{ falls } B < G \\ 360\degree - c & \textrm{ sonst} \end{array} \right. \\
S &=& 1 - \frac{3}{R+G+B} \min (R,G,B) \\
I &=& \frac{1}{3} (R + G + B)
\end{eqnarray*}

\subsubsection{Grauwert-Transformation}

\begin{itemize}
\item $g = (R+G+B)/3$, aber: grün am empfindlichsten -> $$g = 0,299 \cdot R + 0,587 \cdot G + 0,114 \cdot B$$
\end{itemize}

\subsubsection*{Bayer-Pattern\index{Bayer-Pattern}}

\begin{itemize}
\item Hochwertig -> 3 Chips / Pixel, Niederwertig -> 1 Chip/Pixel (R, G, \underline{oder} B)
\begin{itemize}
\item Um nach RGB24 zu konvertieren, muss interpoliert werden.
\item Empfindlichkeit um Faktor 3 niedriger
\end{itemize}
\end{itemize}


\subsubsection*{Lochkameramodell\index{Lochkameramodell}}

\mypic{8}{lochkamera2}

Projektion eines Szenenpunktes $P = (X,Y,Z)$ auf einen Bildpunkt $p = (u,v,w)$ mit Brennweite $f$:
$$\frac{-u}{f} = \frac{X}{Z} \quad , \quad \frac{-v}{f} = \frac{Y}{Z} \quad , \quad w = -f \qquad \Rightarrow \qquad X = - \frac{uZ}{f} \quad , \quad - \frac{vZ}{f}$$
$$p = \left( \begin{array}{c} u \\ v \\ w \end{array} \right) = \left( \begin{array}{c} u \\ v \\ -f \end{array} \right) = - \frac{f}{Z} \left( \begin{array}{c} X \\ Y \\ Z \end{array} \right) = - \frac{f}{Z} P$$
Bei der Projektion geht die $Z$-Komponente verloren! \\
Beispiel mit 2 Kameras:
\mypic{8}{lochkamera1}

\subsubsection*{Lochkameramodell in Positivlage (Mattscheibenmodell)\index{Mattscheibenmodell}}

Einziger Unterschied Mattscheibenmodell $\Leftrightarrow$ Lochkameramodell:
\begin{itemize}
\item Projektionszentrum $C$ liegt hinter der Bildebene
\item dadurch: keine Spiegelung (Minuszeichen entfallen)
\end{itemize}

%\subsubsection*{Linsensysteme\index{Linsensysteme}}
%
%Moderne Kameras benutzen Linsen. \\ Vorteil:
%\begin{itemize}
%\item mehr Lichteinfall
%\end{itemize}
%Nachteile:
%\begin{itemize}
%\item nur Teile der Szene können gleichzeitig fokussiert werden
%\item Linsenkrümmung führt zu Bildverzerrungen
%\end{itemize}
%Bild eines Objekts in Tiefe $Z$ wird in Abstand von der Linse $Z'$ gebildet mit: $$\frac{1}{Z} + \frac{1}{Z'} = \frac{1}{f} \qquad f \, : \, \textrm{Brennweite}$$
%\begin{itemize}
%\item Bei Einstellung der Bildebene auf $Z_0'$ wird ein gewisser Objektbereich um $Z_0$ "{}ausreichend scharf"{} abgebildet.
%\item Beim Menschen: Änderung der Linsenbrennweite.
%\item Bei Kameras: Verschieben der Linse gegenüber Bildebene
%\item Vereinfachung wegen $Z >> Z'$: \\ $\Rightarrow$ Perpektivgleichungen aus dem Lochkameramodell können weiterverwendet werden! $$\frac{1}{Z} + \frac{1}{Z'} \thickapprox \frac{1}{Z'} \quad \Rightarrow \quad \frac{1}{Z'} \thickapprox \frac{1}{f}$$
%\end{itemize}

\section{Bildverarbeitung}

\subsubsection*{Konzepte}

\begin{itemize}
\item Homogene Punktoperationen, Histogrammauswertung, Filterung, Geometrische Operatoren
\end{itemize}
$\to$ Unterdrückung von Störungen, "{}Verschönern"{} von Bildern, Verformen von Bildern

\subsubsection*{Homogene Punktoperatoren\index{Homogene Punktoperatoren}}

Anwendung: $$Img'(u,v) = f(Img(u,v))$$
Unabhängig von der Position bzw. den Nachbarn des Pixels. Implementierung $f$ oftmals als Look-Up-Table (Hardware).

\subsection{Affine Punktoperatoren\index{Affine Punktoperatoren}}

\begin{eqnarray*}
f \, : \, [0 \dots q] &\to& [0 \dots q] \\ x &\mapsto& ax+b
\end{eqnarray*}
Parameter $a$ und $b$ legen die Funktion fest. Anwendungen:
\begin{itemize}
\item Kontrasterhöhung (-verminderung): $b=0$, $a > 0 (a < 0)$
\item Helligkeitserhöhung (-verminderung): $b > 0 (b < 0)$, $a = 1$ 
\item Invertierung: $b = q$, $a = -1$
\end{itemize}

\subsection{Nicht-Affine Punktoperationen\index{Nicht-Affine Punktoperationen}}

Beliebige Abbildungsfunktion $$f \, : \, [0 \dots q] \to [0 \dots q]$$
Anwendung:
\begin{itemize}
\item Ausgleich von Sensor-Nichtlinearitäten, Gewichtung, Binarisierung
\end{itemize}

\subsection{Automatische Kontrastanpassung}

\subsubsection{Spreizung (affin) \index{Spreizung}}
\begin{itemize}
\item [min,max] Intensität linear auf [0,255] abbilden
\item nicht robust, nur unter optimalen Bedingungen
\end{itemize}
\[Img'(u,v) =  \frac{q}{max-min}*Img(u,v)-\frac{q*min}{max-min}\]

\subsubsection{Histogramme (affin) \index{Histogramme}}

Histogrammfunktion: Häufigkeit eines Selektionsmerkmals (Grauwert)

Histogrammdehnung\index{Histogrammdehung}:
\begin{itemize}
\item Verbesserung der Spreizung, Quantile verwendet
\item Akkumuliertes Histogramm berechnen: \[H_a(x):=\sum\limits_{k=0}^x H(k)\]
\item \(H_q(p_{min})\) und \(H_q(p_{max})\) (z.B. \(p_{min} = 0.1\) und \(p_{max} = 0.9\)) bestimmen mit: \[H_q(p) := \inf\{ x \in \{ 0, \dots ,q\}:H_a(x) \geq p \cdot H_a(q) \}\]
\item Falls \(H_q(p_{min}) = H_q(p_{max})\): Bild ist homogen
\item Sonst:
\[ a:=\frac{q}{H_q(p_{max}) - H_q(p_{min})} \]
\[ b:=-\frac{q*H_q(p_{min})}{H_q(p_{max}) - H_q(p_{min})} \]
\end{itemize}

Histogrammausgleich (nicht affin)\index{Histogrammausgleich}:
\begin{itemize}
	\item Kontrasterhöhung in Bereichen, in denen ein Grauwert oft vertreten ist. Dies macht diese Bereiche besser für das Auge sichtbarer
	\item kann in Praxis auch zu Kontrastverminderung führen
\end{itemize}
\verb|for| $x$ \verb|:=| $0$ \verb|to| $q$ \verb|do| \\
\verb|  |$H_n(x)$ \verb|:=| round($\frac{q \cdot H_a(x)}{H_a(q)}$) \\
\verb|endfor|

\subsection{Bildanalyse durch Frequenzanalyse}

\begin{itemize}
\item (Grauwert-) Bilder lassen sich signaltheoretisch als Summe verschiedenfrequenter Signale betrachten.
\item Niedrige Frequenzen: Schwache Grauwertübergänge
\item Hohe Frequenzen: Scharfe Grauwertübergänge
\item Nützlich z.B. zum Finden gerader Linien
\end{itemize}
$\to$ Fourier-Analyse!

\subsubsection*{2-Dim Fouriertransformation}

Kontinuierliches 2-dimensionales Signal: $$F(u,v) = \int\limits_{x = - \infty}^{\infty} \int\limits_{y = - \infty}^{\infty} f(x,y) e^{-2i\pi (ux + vy)} dxdy$$
Diskretes 2-dimensionales Signal: $$F(u,v) = \sum\limits_{x = - \infty}^{\infty} \sum\limits_{y = - \infty}^{\infty} f[x,y] e^{-2i\pi (ux + vy)T}$$

\subsubsection*{Fouriertransformation in der Bildverarbeitung}

DFT bei Bild mit ImgSize $[0 \leq x \leq M][0 \leq y \leq N]$: $$F(u,v) = \sum\limits_{y=0}^{N-1} \left( \sum\limits_{x=0}^{M-1} f[x,y] e^{-2i\pi \frac{ux}{M}} \right) \cdot e^{-2i\pi \frac{vy}{N}}$$
DFT bei quadratischem Bild mit ImgSize $[0 \leq x \leq N][0 \leq x \leq N]$: $$\sum\limits_{y=0}^{N-1} \sum\limits_{x=0}^{N-1} f[x,y] e^{\frac{-2i\pi (ux + vy)}{N}}$$
Anschaulich: Durchführung der 1D-DFT auf jeder Zeile und Speicherung der Daten in Matrix (inndere Klammer). Durchführung der 1D-DFT auf jeder Spalte der ermittelten Matrix.

\begin{itemize}
\item Gewichtete Summation aller Bildpunkte
\item Zerlegung des Bildes in Sinus- und Cosinusfunktionen
\item Je weiter ein Punkt im Spektrum vom Bildmittelpunkt entfernt ist, desto höher ist seine darstellende Frequenz $u$ bzw. $v$.
\item D.h. im Bildinneren tiefe Frequenzen, in äußeren Bereichen hohe Frequenzen
\item Anwendung:
\begin{itemize}
\item Bildanalyse (z.B. Muster-, Geschwindigkeitserkennung)
\item Bildfilterung (z.B. Tiefpass)
\item Bildkompression (z.B. in jpeg Format)
\end{itemize}
\end{itemize}

\subsubsection*{Fouriertransformation}

\begin{itemize}
\item Die Variablen $u$ und $v$ heissen Frequenzvariablen
\item $F(u,v)$ ist komplexe Funktion
\item $F(u,v)$ ist darstellbar als 2 Bilder
\begin{itemize}
\item in Realteil und Imaginärteil $$F(u,v) = R(u,v) + I(u,v)$$
\item oder Betrag (auch Spektrum genannt, oft logarithmisch dargestellt) und Phase $$F(u,v) = |F(u,v)| \cdot e^{i \varphi (u,v)}$$
\end{itemize}
\item Quadrat des Spektrums heisst spektrale Dichte.
\end{itemize}

\subsection{Bildbearbeitung}

\begin{itemize}
\item Durch Filter im Ortsbereich oder Transferfunktionen im Frequenzbereich.
\item Ortsbereich:
\begin{itemize}
\item Manipulation von Grauwerten
\item anschaulich
\item häufig: Punktoperationen, Glättung
\end{itemize}
\item Frequenzbereich:
\begin{itemize}
\item Manipulation der Frequenzanteile
\item keine unmittelbare bildliche Vorstellung
\item häufig: starke Glättung, frequenzselektive Filter
\end{itemize}
\end{itemize}

\subsubsection*{Bildbearbeitung im Ortsbereich}

Faltung zweier Funktionen 1D kontinuierlich: $$h(x) = f(x) * g(x) = \int\limits_{a = - \infty}^{\infty} f(a) g(x-a) da$$
Faltung zweier Funktionen 1D zeitdiskret: $$h[x] = f[x] * g[x] = \sum\limits_{a = - \infty}^{\infty} f[a] g[x-a]$$
Faltung zweier Funktionen 2D kontinuierlich: $$h(x,y) = f(x,y) * g(x,y) = \int\limits_{a = -\infty}^{\infty} \int\limits_{b = -\infty}^{\infty} f(a,b) g(x-a,y-b) dadb$$
Faltung zweier Funktionen 2D zeitdiskret: $$h[x,y] = f[x,y] * g[x,y] = \sum\limits_{a = -\infty}^{\infty} \sum\limits_{b = -\infty}^{\infty} f[a, g[x-a,y-b]$$

\subsubsection*{Faltung}

\begin{itemize}
\item Bildmatrizen werden in den relevanten Randbereichen mit Nullen gefüllt.
\item Der neue Bildwert ist eine gewichtete Summe der Pixel die unter der gespiegelten Matrix liegen.
\item Als Gewichte dienen die Matrizenwerte.
\item Bildfilterung ist die Faltung einen Bildes mit einer Filtermatrix bzw. Maske.
\item Die Transferfunktion $H(u,v)$ ist die Fouriertransformierte der Filterfunktion $h(x,y)$.
\end{itemize}


Beispiel Sobel-X Filters:
\mypic{12}{filterbsp}

\subsubsection*{Filteroperationen}

\begin{itemize}
\item Tiefpassfilter: Glättung, Rauschelimination
\begin{itemize}
	\item Mittelwertfilter
	\item Gauß-Filter
\end{itemize}
\item Hochpassfilter: Kantendetektion
\begin{itemize}
	\item Prewitt
	\item Sobel
	\item Roberts
	\item Laplace
\end{itemize}
\item Kombinierte Operationen: Laplacian of Gaussian
\end{itemize}

\subsection{Filter}

\subsubsection{Mittelwertfilter\index{Mittelwertfilter}}
\begin{itemize}
	\item Ziel: Rauschunterdrückung \\ Beispiel: Durchschnitt aus 8-Umgebung und Punkt. Größe beliebig wählbar.
  \item $$m'(x,y) = \sum\limits_{j=-1}^1 \sum\limits_{i=-1}^1 m(i,j) \cdot p(x-j,y-i)$$
\end{itemize}

\subsubsection{Gauß-Filter\index{Gauß-Filter}}
\begin{itemize}
  \item Ziel: Rauschunterdrückung, Glättung
  \item Definiert durch zweidimensionale Gauß-Funktion
	
	\begin{itemize}
		\item Ortsbereich: $f(x,y) = \frac{1}{2 \pi \sigma^2} e^{-\frac{x^2 + y^2}{2\sigma^2}}$
		\item Frequenzbereich: $F(u,v)=e^{-\frac{u^2 + v^2}{2}\sigma^2}$
	\end{itemize}
	
	\item Approximation von $f(x,y)$ durch einen 3x3-Filter für $\sigma=0.85$: $F_{Gauß}=\frac{1}{16} \left(
	\begin{array}{ccc}
	1 & 2 & 1 \\
	2 & 4 & 2 \\
	1 & 2 & 1
	\end{array} \right)
	$
	
	\item Die Stärke der Glättung ist ausschließlich durch den Parameter $\sigma$ bestimmt (je größer $\sigma$ desto stärker die Glättung)
	\item Die Größe nxn beeinflusst die Güte der Approximation des Filters
	\item Im Ortsbereich kann als Faustregel für eine ausreichende Approximation $n=2\sigma \cdot 2 +1 $ verwendet werden
	\item Deshalb: Für eine starke Glättung lohnt sich die Anwendung im Frequenzbereich
\end{itemize}



\subsubsection{Prewitt\index{Prewitt-Filter}}

\textbf{Prewitt-X Filter} $$P_x = \frac{\partial g(x,y)}{\partial x}$$ approximiert durch $$p_x = \left( \begin{array}{ccc} -1 & 0 & 1 \\ -1 & 0 & 1 \\ -1 & 0 & 1 \end{array} \right)$$
Kantendetektion: vertikal gut, horizontal schlecht \\
\textbf{Prewitt-Y Filter} $$P_y = \frac{\partial g(x,y)}{\partial y}$$ approximiert durch $$p_y = \left( \begin{array}{rrr} -1 & -1 & -1 \\ 0 & 0 & 0 \\ 1 & 1 & 1 \end{array} \right)$$
Kantendetektion: vertikal schlecht, horizontal gut \\
\textbf{Prewitt-Operator}
\begin{itemize}
\item Kombination der Prewitt-Filter zur Bestimmung des Grauwertgradientenbetrages $M$: $$M \thickapprox \sqrt{P_x^2 + P_y^2}$$
\item Danach: Schwellwertfilterung
\end{itemize}

\subsubsection{Sobel\index{Sobel-Filter}}

\textbf{Sobel-X Filter} $$S_x = \frac{\partial g(x,y)}{\partial x}$$ approximiert durch $$s_x = \left( \begin{array}{ccc} -1 & 0 & 1 \\ -2 & 0 & 2 \\ -1 & 0 & 1 \end{array} \right)$$
Kantendetektion: vertikal gut, horizontal schlecht \\
\textbf{Sobel-Y Filter} $$S_y = \frac{\partial g(x,y)}{\partial y}$$ approximiert durch $$s_y = \left( \begin{array}{rrr} -1 & -2 & -1 \\ 0 & 0 & 0 \\ 1 & 2 & 1 \end{array} \right)$$
Kantendetektion: vertikal schlecht, horizontal gut \\
\textbf{Sobel-Operator}
\begin{itemize}
\item Kombination der Sobel-Filter zur Bestimmung des Grauwertgradienten-Betrages $M$: $$M \thickapprox \sqrt{S_x^2 + S_y^2}$$
\item Danach: Schwellwertfilterung
\end{itemize}

\subsubsection{Roberts\index{Roberts-Filter}}

$$R(g(x,y)) = |R_x(g(x,y))| + |R_y(g(x,y))|$$ wobei $$R_x = \left( \begin{array}{rr} -1 & 0 \\ 0 & 1 \end{array} \right) \quad , \quad R_y = \left( \begin{array}{rr} 0 & -1 \\ 1 & 0 \end{array} \right)$$
Kantendetektion: diagonal gut

\subsubsection*{Laplace\index{Laplace-Filter}}

Laplace-Operator: $$\nabla^2 g(x,y) = \frac{\partial^2 g(x,y)}{\partial x^2} + \frac{\partial^2 g(x,y)}{\partial y^2}$$ wobei $$\nabla^2 \thickapprox \left( \begin{array}{rrr} 0 & 1 & 0 \\ 1 & -4 & 1 \\ 0 & 1 & 0 \end{array} \right)$$
Kantendetektion: Nulldurchgänge markieren Kanten, Subpixelgenauigkeit erreichbar \\ Näherung des Laplace-Operators: $$\nabla^2 \thickapprox \left( \begin{array}{rrr} 1 & 4 & 1 \\ 4 & -20 & 4 \\ 1 & 4 & 1 \end{array} \right)$$
Kantendetektion: Stärkere Kanten, aber mehr Störkanten

\subsubsection{Laplacian of Gauß (LoG)\index{Laplacian of Gauß-Filter}}

Der Laplace-Operator ist gegen Rauschen sehr empfindlich. Wesentlich bessere Ergebnisse erhält man, wenn man das Bild zunächst mit einem Gauß-Filter glättet und danach den Laplace-Operator anwendet. $$LoG(g(x,y)) = \nabla^2 (G(x,y) * g(x,y))$$ Approximation (Faltung mit Matrix): $$\nabla^2 G(x,y) = \left( \begin{array}{rrrrr} 0 & 0 & -1 & 0 & 0 \\ 0 & -1 & -2 & -1 & 0 \\ -1 & -2 & 16 & -2 & -1 \\ 0 & -1 & -2 & -1 & 0 \\ 0 & 0 & -1 & 0 & 0 \end{array} \right)$$
Kantendetektion: Stärkere Kanten,weniger Rauschen

\subsubsection{Canny-Kantendetektor\index{Canny-Kantendetektor}}
\begin{itemize}
	\item Ziel: Der optimale Kantendetektor
	\begin{itemize}
		\item Gute Detektion
		\item Gute Lokalisierung
		\item Minimale Antwort ("`dünne Linien"')
	\end{itemize}
	\item Canny-Kantendetektor liefert binäre Antwort
	\begin{itemize}
		\item 0: keine Kante
		\item 255: Kante
	\end{itemize}
	\item Subpixelgenauigkeit durch Erweiterung möglich
	\item Algorithmus besteht aus mehreren Schritten
\end{itemize}
\begin{enumerate}
\item Rauschunterdrückung: Gauß-Filter
\item Berechnung der Gradienten in horizontaler und in vertikaler Richtung mit Prewitt oder Sobel
	\begin{itemize}
	\item Berechnung der Richtung: $\phi=atan(\frac{g_y}{g_x})$
	\item Einteilung der Richtung in vier Quadranten:
	\begin{enumerate}
		\item $[-67,5°,-22,5°)$
		\item $[-22.5°,22.5°)$
		\item $[22.5°,67.5°)$
		\item $[-90°,-67.5°)$ oder
		\item $[67.5°,90°]$
	\end{enumerate}
	\end{itemize}
\item Non-Maximum Surpression:
	\begin{itemize}
	\item Für jeden Pixel in Gradientenrichtung schauen, ob das Pixel davor oder dahinter einen höheren Wert hat. Falls ja, dann Pixel auf 0 setzen, falls nein, dann beibehalten.
	\end{itemize}
\item Hysterese-Schwellwerverfahren:
	\begin{itemize}
	\item Verwende zwei Schwellwerte \(T_1\) und \(T_2\) mit \(T_1 \leq T_2\)
	\item Markiere alle Pixel mit Werten größer \(T_2\) als Kantenpixel
	\item Setze alle Pixel mit Werten kleiner \(T_1\) auf 0
	\item Beginnend bei jedem Kantenpixel:
		\begin{itemize}
		\item Verfolge alle angrenzenden Kanten, solange Wert \(\geq T_1\)
		\item Markiere alle dazugehörigen Pixel als Kanten
		\end{itemize}
	\item Setze alle noch nicht als Kante markierten Pixel auf 0
	\end{itemize}
\end{enumerate}


\section{2D Bildverarbeitung}

\subsubsection*{Sensorische Erfassung}

Aufgaben der sensorischen Umwelterfassung:
\begin{itemize}
\item Wiedererkennung bekannter Sachverhalte: Objekte, Personen, Orte
\item Erlernen neuer Sachverhalte
\item Erkennung der eigenen Bewegung
\end{itemize}
Verfahren zur Lösung dieser Aufgaben:
\begin{itemize}
\item Sensorische Primitive, Segmentierung
\item Annahmen, Einschränkungen
\item Lernverfahren
\end{itemize}

\subsection{Segmentierung\index{Segmentierung}}

Segmentierung ist die Aufteilung eines Bildes in aussagekräftige Bestandteile. Erlaubt:
\begin{itemize}
\item Aussagen über das Bild
\item Reduktion der Datenmenge
\item Verfolgung von Merkmalen über die Zeit / mehrere Sensoren
\end{itemize}
Beliebt sind: Kanten, Ecken, Textur, Farbe

\subsubsection{Schwellwertfilterung\index{Schwellwertfilterung}}

Schwellwertfilterung zur Konvertierung eines Grauwertbildes in ein binäres Bild. Ziel: Trennung interessanter Objekte vom Hintergrund. $$Img'(u,v) = \left\{ \begin{array}{cl} q & \textrm{ falls } Img(u,v) \geq T \\ 0 & \textrm{ sonst} \end{array} \right.$$

\subsubsection{Farbe}

Oft können Objekte über ihre Farbe segmentiert werden:
\begin{itemize}
\item menschliche Hautfarbe
\item einheitlich gefärbte Objekte
\end{itemize}
Problem:
\begin{itemize}
\item wechselnde Lichtbedingungen
\item Reflexionen, Schattenwürfe
\end{itemize}
Verfahren:
\begin{itemize}
\item Histogrammbasiert (z.B. in RGB, HSV bzw. RG, HS) \\ HS-Farbhistogramm:
\begin{itemize}
\item Weglassen des $I$-Kanals ergibt 2D-Histogramm
\item Training eines Klassifikators auf dem Histogramm
\end{itemize}
\item mit Hilfe der Mahalanobis-Distanz (z.B. in RGB): \\ gegeben: $x_i = (R,G,B)^T$ sind manuell positiv klassifizierte Pixel
\begin{eqnarray*}
C &=& \frac{1}{n-1} \sum\limits_{i=1}^n (x_i - \overline{x})(x_i - \overline{x})^T \qquad \textrm{Kovarianzmatrix} \\ p(x) &=& e^{- \frac{1}{2 \sigma^2} x^T C^{-1} x} \qquad \textrm{Berechnung der Farbwahrsch.}
\end{eqnarray*}
\item Klassifikation durch Verwendung von Neuronalen Netzen
\item durch Intervallschranken im HSI-Farbraum:
$$f(H,S,V) = H \geq H_{\min} \wedge H \leq H_{\max} \wedge S \geq S_{\min} \wedge S \leq S_{\max} \wedge V \geq V_{\min} \wedge V \leq V_{\max}$$
\end{itemize}

\subsection{Morphologische Operatoren\index{Morphologische Operatoren}}

\begin{itemize}
\item Wähle Struckturelement, z.B. Quadrat mit 3x3 Pixel
\item Wandere mit Struckturelement über Bild und führe Operation durch
\item \textbf{Dilatation}\index{Dilatation} ist ein Operation auf Binärbildern. Alle Bereiche, die farbig (1 und nicht 0, da Binär) sind, werden ausgedehnt. Immer wenn unter dem mittleren Pixel des Struckturelements ein farbiges Pixel ist, werden alle Pixel unter dem Struckturelement als farbig markiert.
\item \textbf{Erosion}\index{Erosion} ist die Komplemetäroperation zu Dilatation. Nur falls alle Pixel unter dem Struckturelement farbig sind, wird das Element in der Mitte farbig gelassen und alle anderen auf 0 gesetzt. Falls nicht alle Pixel farbig sind, werden alle auf 0 gesetzt.
\item Öffnen-Operation\index{Öffnen-Operation}: Zuerst Erosion und anschließend Dilatation anwenden.
\item Schließen-Operation\index{Schließen-Operation}: Zuerst Dilatation und anschließend Erosion anwenden.

\end{itemize}


\subsubsection{Bewegung}

Einfacher Ansatz: Differenzbilder
\begin{itemize}
\item Subtraktion aufeinander folgender Bilder einer Video-Sequenz: $$Img_t'(u,v) = |Img_t(u,v) - Img_{t-1}(u,v)|$$
\item Anschließend kann auf $Img_t'$ Schwellwertfilterung durchgeführt werden
\item Regionen, in denen sich etwas bewegt, erscheinen weiß; ruhige Regionen erscheinen schwarz
\item Bewegung in homogenen Regionen wird nicht erkannt (Kanten, Textur sind notwendig)
\item Richtung der Bewegung wird nicht erkannt
\end{itemize}
Differenzbilder werden auch für Hintergrundsubtraktion verwendet, dann wird $Img_{t-1}$ durch ein festes $Img_0$ ersetzt. Weitere Ansätze: Optical Flow und Erweiterungen

\subsubsection{Region Growing\index{Region Growing}}

gegeben: Graustufen-Bild, gesucht: zusammenhängende Regionen \\ Algorithmus in Pseudocode:
\begin{enumerate}
\item wähle Saatpunkt $p_0 = (u_0,v_0)$
\item initialisiere Region $R = \{ p_0 \}$, wähle Schwelle $\varepsilon$
\item solange $\exists p \in R$, $q \not\in R$ mit $||p-q|| \leq 1$ und $|Img(p_0) - Img(q)| \leq \varepsilon$ mache $R = R \cup \{ q \}$
\end{enumerate}

\subsubsection{Kanten}

Vom Menschen konstruierte Umgebungen:
\begin{itemize}
\item gut strukturiert
\item viele gerade Linien (Wände, Türen, Schränke)
\item einfache Segmentierung / 3D-Rekonstruktion
\item viele Informationen in einem einzigen Merkmal
\end{itemize}
Vorgehensweise: $$\textrm{Bildaufnahme} \quad \Rightarrow \quad \textrm{Filtern, Binarisieren} \quad \Rightarrow \quad \textrm{Pixel } \to \textrm{ Kantensegmente}$$

\subsubsection*{Pixel $\to$ Kanten}

\textbf{Iterative Endpoint Fit:}\index{Iterative Endpoint Fit} \\[0,1cm]
gegeben: Punkte $P$, Linien $L = \{ \}$, Distanzschwelle $d$
\begin{itemize}
\item Fine $x_1$, $x_2$ aus $P$ mit $||x_1 - x_2|| = \max$; \\ verbinde sie durch Linie $l_0 = \{x_1,x_2\}$; $L=L \cup \{l_0\}$
\item Entferne $x_1$, $x_2$ aus $P$
\item Für alle $l \in L$:
\begin{itemize}
\item Finde $x \in P$ mit $||l-x|| = \max$
\item Wenn $||l-x|| < d$:
\begin{itemize}
\item Ordne $x$ als Mitgliedpunkt $l$ zu
\item Entferne $x$ aus $P$
\end{itemize}
\item Sonst
\begin{itemize}
\item Brich $l$ in $l_1 = \{ x_1 , x\}$ und $l_2 = \{x , x_2 \}$ auf
\item Andere Mitgliedspunkte von $l$ wieder in $P$
\end{itemize}
\item $P$ leer $\Rightarrow$ Abbruch, sonst weiter
\end{itemize}
\item Lösche Linien mit weniger als $n$ Punkten
\end{itemize}

\textbf{Hough-Transformation:}\index{Hough-Transformation}
\begin{itemize}
\item Ziel: Erkennung gerader Linien im Bild
\item Ansatz: Stelle Linie durch Normalenvektor (Länge, Winkel) in Polarkoordinaten dar (Sinus-Kosinus-Kurve)
\item Kurven für kollineare Punkte schneiden sich in genau zwei Punkten $$r = x \cdot \cos(\theta) + y \cdot \sin(\theta)$$
\item Transformation in den Hough-Raum: Additives Eintragen aller Sinus-Kosinus-Kurven für alle Pixel in ein Histogramm
\item Finden der Maxima bzw. Cluster von "{}Treffern"{} im Hough-Raum
\item Brute-Force-Ansatz; für ein $n \times n$-Bild liegt die Laufzeit in $O(n^3)$
\end{itemize}
Unterschied zur Regressionsanalyse: Die Bestimmung der Regressionsgerade ist das geeignetere Verfahren, wenn schon klar ist, welche Pixel eine Gerade bilden sollen; dann ist die Regressionsgerade die optimale Gerade im Sinne der Summe der Fehlerquadrate. Die Regressionsgerade kann jedoch keine Segmentierung vornehmen. Dahingegen lassen sich mit der Hough-Transformation in beliebigen Bildern geradlinige Strukturen berechnen; die punkte dazu müssen jedoch verhältnismäßig exakt auf einer Linie liegen.

\subsection{Punktmerkmale}

Kanten/Konturen/Farbe können nicht immer für die Segmentierung herangezogen werden. Texturierte Objekte lassen sich in der Regel nicht durch die bislang vorgestellten Verfahren segmentieren. Lösung: Verwendung von \textsl{lokalen} Punktmerkmalen (auch genannt: Textmerkmale):
\begin{itemize}
\item Harris Corner Detector
\item Shi-Tomasi Features
\item SIFT-Features
\item Maximally Stable Extremal Regions
\end{itemize}
Punktmerkmal: $(2n+1) \times (2n+1)$-Pixel-Block um Pixel $p$. Fast immer basierend auf Grauwertbildern. Gewünschte Eigenschaft: Wiedererkennbarkeit \\ $\Rightarrow$ hoher Gradient in mehrerer Richtungen \\[0,1cm]
\textbf{Harris Corner Detector}\index{Harris Corner Detector}: \\
Sind die Eigenwerte der Matrix $$A = \left( \begin{array}{cc} \left( \frac{\partial Img(x,y)}{\partial x} \right)^2 & \frac{\partial Img(x,y)}{\partial x} \frac{\partial Img(x,y)}{\partial y} \\ \frac{\partial Img(x,y)}{\partial x} \frac{\partial Img(x,y)}{\partial y} & \left( \frac{\partial Img(x,y)}{\partial y} \right)^2 \end{array} \right)$$ groß, dann verursacht eine kleine Bewegung in beliebiger Richtung eine große Grauwertänderung. \\ Finden von Ecken durch Suche nach lokalen Maxima in: $$R = det(A) - k \cdot trace(A)^2 \quad , \quad k \thickapprox 0,04$$
Häufiges Problem:
\begin{itemize}
\item Wiederfinden bzw. Zuordnung von Punktmerkmalen für:
\begin{itemize}
\item Objekterkennung auf der Basis von Punktmerkmalen
\item Stereo-Sehen bzw. "{}Structure from Motion"{} (Korrespondenzproblem)
\end{itemize}
\item Lösung des Korrespondenzproblems erfolgt für Punktmerkmale durch Korrelationsverfahren:
\begin{itemize}
\item \textbf{Sum of Squared Differences}\index{Sum of Squared Differences} (SSD) wird minimal bei guter Übereinstimmung: $$\sum\limits_{i = -n}^n \sum\limits_{j = -n}^n (Img_0(x-i,y-j) - Img_1(x-i,y-j))^2$$
\item \textbf{Sum of Absolute Differences}\index{Sum of Absolute Differences} (SAD) wird minimal bei guter Übereinstimmung: $$\sum\limits_{i = -n}^n \sum\limits_{j = -n}^n |Img_0(x-i,y-j) - Img_1(x-i,y-j)|$$
\item \textbf{(Zero Mean) Cross Correlation}\index{(Zero Mean) Cross Correlation} wird maximal bei guter Übereinstimmung: $$\sum\limits_{i = -n}^n \sum\limits_{j = -n}^n (Img_0(x-i,y-j)- \overline{Img_0})(Img_1(x-i,y-j)- \overline{Img_1})$$ wobei $$\overline{Img} \, : \, \textrm{Durchschnitt}$$
\item \textbf{Zero Mean Normalized Cross Correlation}\index{Zero Mean Normalized Cross Correlation} wird maximal bei guter Übereinstimmung: $$\frac{\sum\limits_{i=-n}^{n}\sum\limits_{j=-n}^{n}Img_1(u_1 + i, v_1 + j) - \overline{Img_1}(u_1,v_1,n)) \cdot (Img_2(u_2 + i, v_2 + j) - \overline{Img_2}(u_2,v_2,n))}{\sqrt{\sum\limits_{i=-n}^n \sum\limits_{j=-n}^n (Img_1(u_1 + i, v_1 + j) - \overline{Img_1}(u_1,v_1,n))^2 \sum\limits_{i=-n}^n \sum\limits_{j=-n}^n Img_2(u_1 + i, v_1 + j) - \overline{Img_2}(u_1,v_1,n))^2} }$$
wobei $$\overline{Img}(u,v,n) = \frac{1}{(2n+1)^2} \sum\limits_{i=-n}^n \sum\limits_{j=-n}^n Img(u+i,v+j)$$
\end{itemize}
\end{itemize}
Für die Objekterkennung geschieht das Wiederfinden oftmals unter Verwendung:
\begin{itemize}
\item der Hauptkomponentenanalyse oder engl. \textsl{Principal Component Analysis} (PCA) und Nearest-Neighbor- bzw. $k$-Nearest-Neighbor-Klassifikator
\item von Neuronalen Netzen
\item oder Kombinationen (zuerst PCA zur Kompression, dann SVM (\textsl{Support Vector Machine}) zur Klassifikation)
\end{itemize}
Für die Objekterkennung existieren eine Vielzahl von Repräsentationen von lokalen Merkmalen:
\begin{itemize}
	\item SIFT (Scale Invariant Feature Transformation
	\item SURF (Speeded Up Robust Features)
	\item MSER (Maximally Stable Extremal Regions)
	\item Repräsentation eines Bildausschnittes über eine Vielzahl (100-200) synthetisch generierter Ansichten, Matching über PCA (Principal Component Analysis)
\end{itemize}

\section{Geometrische 2D-Transformationen}

\subsection{Translation}\index{Translation}

Translation eines 2D-Vektors: $$\left( \begin{array}{c} x_0 \\ y_0 \end{array} \right) + \left( \begin{array}{c} x \\ y \end{array} \right) = \left( \begin{array}{c} x_0 + x \\ y_0 + y \end{array} \right)$$

\subsection{Rotation}\index{Rotation}

\begin{itemize}
\item o.B.d.A. auf Einheitsvektor zurückführbar (Basistransformation)
\item Konvention: Rechtskoordinatensystem
\item Rotation von $(x_0,y_0)$ um Winkel $\beta$ mit Ergebnis $(x,y)$:
\end{itemize}
Aus Additionstheorem:
\begin{eqnarray*}
x &=& \cos(\alpha + \beta) = \cos(\alpha) \cos(\beta) - \sin(\alpha) \sin(\beta) \\ y &=& \sin(\alpha + \beta) = \sin(\beta) \cos(\alpha) + \cos(\beta) \sin(\alpha)
\end{eqnarray*}
und mit $(x_0,y_0) = (\cos(\alpha), \sin(\alpha))$:
$$\left( \begin{array}{c} x \\ y \end{array} \right) = \left( \begin{array}{rr} \cos(\beta) & -\sin(\beta) \\ \sin(\beta) & \cos(\beta) \end{array} \right) \left( \begin{array}{c} x_0 \\ y_0 \end{array} \right)$$

\subsubsection*{Homogene Koordinaten}

Homogene Koordinaten $$h = (h_0,h_1, \dots, h_i, h_{i+1})$$ eines Punktes $p$ im $R^i$ mit $$p = (p_0,\dots,p_i)$$ sind Zahlen, für die gilt: $$p_k = \frac{h_k}{h_{i+1}} \quad \forall 0 \leq k \leq i$$

\subsubsection*{Homogene 2D-Transformationen}

Transformationen definiert durch Rotation $R$ und Translation $t$.
$$\myvectwo{x}{y} = R \myvectwo{x_0}{y_0} + t = \myvecfour{r_{11}}{r_{12}}{r_{21}}{r_{22}} \myvectwo{x_0}{y_0} + \myvectwo{t_x}{t_y}$$
Darstellung mit Hilfe homogener Koordinaten und einer geschlossenen Transformationsmatrix:
$$\myvecthree{x}{y}{1} = \left( \begin{array}{cc|c} & R & t \\ \hline 0 & 0 & 1 \end{array} \right) \myvecthree{x_0}{y_0}{1} = \left( \begin{array}{cc|c} r_{11} & r_{12} & t_x \\ r_{21} & r_{22} & t_y \\ \hline 0 & 0 & 1 \end{array} \right) = A \myvecthree{x_0}{y_0}{1}$$

\section{Partikel Filter und 2D-Tracking}
\begin{itemize}
\item Für Tracking-Applikationen wird oftmals das Kalmanfilter, das Partikel Filter oder Kombinationen aus beiden verwendet. Vorteile des Partikel Filters gegenüber dem Kalmanfilter:
\begin{itemize}
\item Es kann automatisch mehrere, parallel existierende Hypothesen halten $\Rightarrow$ geringere Gefahr in lokalen Minima zu verharren.
\item Es kann beliebige Wahrscheinlichkeitsdichten modellieren und dadurch nichtlineare Bewegungen auf natürliche Weise verfolgen.
\end{itemize}
\item Nachteil: Rechenaufwand ist proportional zur Anzahl der notwendigen Partikel, welche (bei konstanter Auflösung) mit der Dimension des Konfigurationsraums exponentiell steigt. 
\item Im Kern des Partikel Filters befinden sich ein Modell mit Konfigurationsraum $R^n$. 
\item \textit{Eingabe:} Beobachtungen $z$; hier: Bilder einer Videosequenz
\item \textit{Ausgabe:} Schätzung der Konfiguration $s \in R^n$, die den aktuellen Beobachtungen $z$ entspricht \item \textit{Zentrale Funktion:} Bewertungsfunktion $p(z|s)$, die die a-posteriori Wahrscheinlichkeit berechnet, dass $s$ die zu $z$ passende Konfiguration ist.
\item Das Partikel Filter modelliert die Wahrscheinlichkeitsdichtefunktion oder engl. \textsl{probability density function (pdf)} durch eine feste Anzahl von $N$ Partikeln. 
\item Die Wahrscheinlichkeitsdichtefunktion beschreibt die a-posteriori Wahrscheinlichkeiten für den Konfigurationsraum. 
\item Jedes Partikel ist ein Paar $(s_i, \pi_i)$, wobei $s_i$ eine Konfiguration ist und $\pi_i$ die dazugehörige a-posteriori Wahrscheinlichkeit mit $\sum_{i=1}^n \pi_i = 1$. 
\item Die aktuelle Schätzung des Partikel Filters erfolgt über das gewichtete Mittel über alle Partikel: $\overline{s} = \sum\limits_{i=1}^{N} \pi_i \cdot s_i$
\end{itemize}
\textbf{\textsl{Algorithmus}}

\begin{enumerate}
\item Initialisiere alle $N$ Partikel (z.B. mit Gleichverteilung)
\item Verarbeite neue Beobachtungen (z.B. Vorverarbeitung neuer Bilder)
\item Ziehe $N$ Partikel aus der letzten Generation, proportional zu ihrer Wahrscheinlichkeit $\pi_i$, und für jedes dieser Partikel:
\begin{itemize}
\item Berechne neue Konfiguration auf Basis der alten Konfiguration durch Addition normalverteilten Rauschens und evtl. durch Hinzunahme eines dynamischen Modells.
\item Berechne für diese neue Konfiguration die neue a-posteriori Wahrscheinlichkeit mit Hilfe der Bewertungsfunktion $\myprob{z}{s}$
\end{itemize}
\item Berechne aktuelle Schätzung $\overline{s}$ über alle Partikel
\item Fahre fort mit Schritt 2
\end{enumerate}

\textbf{\textsl{Einfaches Beispiel für eine Bewertungsfunktion}}

\begin{itemize}
\item Anwendung: \\ 2D-Tracking einer dichten, in etwa quadratischen Fläche von in etwa fester Größe in einem binarisierten Bild
\item Modell: \\ Quadrat fester Größe mit Kantenlänge $k$ mit Konfigurationsraum $R^2$ (Koordinaten $u,v$ im Bild)
\item Bewertungsfunktion: $$\myprob{z}{s} \propto e^{- \frac{1}{2 \sigma^2} \left( k^2 - \sum_{m \in M} g_m \right)}$$ wobei $M$ die Menge aller Pixel im binarisierten Bild $z$ (Beobachtungen) im durch $s$ (Konfiguration) definierten Quadrat beschreibt und $g_m$ deren Intensität aus $\{0,1\}$.
\end{itemize}









